{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "#  torch.cuda.is_available() 함수는 현재 컴퓨터에서 CUDA를 이용할 수 있는 지 알아보는 함수\n",
    "# 오류가 있다면 false 반환\n",
    "# 제대로 설치시 true (CUDA용 파이토치, CUDA도 제대로 설치)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "# torch.device에 CUDA를 지원하면 \"cuda\"를 \n",
    "# 아니면 \"cpu\"를 설정한 후 변수에 저장 \n",
    "# 나중에 텐서와 가중치에 대한 연산을 CPU와 GPU 중 에서 실행할지 결정할 떄 사용 \n",
    "\n",
    "EPOCHS = 50\n",
    "# 전체 반복 횟수\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# FashionMNIST 부터는 데이터가 많아져 한 번에 사용하는 것은 효율적이지 않기 때문에\n",
    "# 여러 개의 미니배치로 잘라서 사용\n",
    "# 각 미니배치의 크기는 64개 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d8cd25eea44f680421ba1d61ba258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0258eeee88444398b2f18e8f50316914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647399897f8e426ca6aa245089979a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aead2566f6154f5aa5b7f9c7405ee5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## DataLoader 사용 \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST( './.data/',\n",
    "                            train = True,\n",
    "                            download = True,\n",
    "                            transform = transforms.Compose([\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,),(0.3081,))\n",
    "                            ])),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST( './.data/',\n",
    "                            train = False,\n",
    "                            transform = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1307,),(0.3081,))\n",
    "                            ])),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이미지 분류를 위한 인공신경망 구현 입력 X와 레이블 Y를 입력 받아 \n",
    "## 학습한 다음 새로운 X가 왔을 때 어떤 패션 아이템인지 예측한다. \n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dropout_p = 0.2): # 선형결합을 수행하는 객체를 만든다. \n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256) #픽셀값 784개를 입력받아 256 출력 \n",
    "        self.fc2 = nn.Linear(256, 128)#픽셀값 256개를 입력받아 128 출력 \n",
    "        self.fc3 = nn.Linear(128,10) #픽셀값 128개를 입력받아 10 출력\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,784) #view( ) 로 1차원 행렬을 만든다. \n",
    "        x = F.relu(self.fc1(x)) # fc1()를 거쳐 relu( )활성화 함수 \n",
    "        \n",
    "        #드롭아웃 추가 \n",
    "        x = F.dropout(x, training = self.trianing,\n",
    "                     p = self.dropout_p)\n",
    "        x = F.relu(self.fc2(x)) # fc2()를 거쳐 relu( )활성화 함수\n",
    "        \n",
    "        # 드롭 아웃 추가 \n",
    "        x = F.dropout(x, training = self.trianing,\n",
    "                     p = self.dropout_p)\n",
    "        x = self.fc3(x) # fc3()를 거침 \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(DEVICE) \n",
    "# to 함수로 연산을 어디서 수행할지 결정 \n",
    "# CPU 사용시 to gkatn t0 함수 사용할 필요 x\n",
    "# to(\"cuda\") ->  GPU 사용 \n",
    "# DEVICE 변수를 설정했기 때문에 \n",
    "#자동으로 CUDA를 사용할 경우 GPU로  아닐경우 CPU사용 \n",
    "optimizer = optim.SGD(model.parameters(), lr =0.01) \n",
    "# 최적화 알고리즘으로 파이토치 내장 모듈인 optim.SGD 사용 \n",
    "# 모델 내부의 정보를 넘겨주는 model.parameters() 함수와 \n",
    "# 임의로 설정한 학습률(lr) = 0.01 입력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습에 들어가는 모든 연산 \n",
    "\n",
    "# model- 학습에 필요한 모든 요소 \n",
    "# train_loader - 데이터 공급\n",
    "# optimizer - 최적화 담당 \n",
    "\n",
    "\n",
    "# 모델에 입력하는 data = [배치크기, 색, 높이, 넓이] = [64,1,28,28 ]\n",
    " \n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train() #학습모드로 바꾼다.\n",
    "    \n",
    "    \n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        # 앞서 모델의 가중치를 GPU로 보냈다면 \n",
    "        # 학습 데이터도 같은 장치로 보내야 연산을 수행\n",
    "        optimizer.zero_grad()\n",
    "        # 반복 시마다 기울기를 계산 \n",
    "        output = model(data)\n",
    "        # 학습 데이터에 대한 모델의 예측값을 받아온다. \n",
    "        loss = F.cross_entropy(output,target)\n",
    "        # 앞 장에서는 클래스가 2개 라 이진 교차 엔트로피를 사용하였지만\n",
    "        #이번에는 10개가 존재하기 때문에 교차 엔트로피 사용 \n",
    "        # 앞 장 처럼 criterion() 함수를 만들 수 있으나\n",
    "        # 가중치를 보관할 필요가 없으므로 \n",
    "        # torch.nn.functional 의 cross_entropy() 함수를 직접 사요 ㅇ\n",
    "        loss.backward()\n",
    "        # 기울기 가 계싼 \n",
    "        optimizer.step()\n",
    "        # 계싼한 기울기를 앞 서 정의한 알고리즈메 맞춰 가중치를 수정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    # 평가 모드로 바꾼다. \n",
    "    test_loss = 0 \n",
    "    # 테스트 오차 수 \n",
    "    correct = 0 \n",
    "    # 테스트 예측이 맞은 수 \n",
    "    \n",
    "    with torch.no_grad():# 평가 과정에서는 기울기를 계산하지 않아도 도니다. \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            # 학습 때와 같이 데이터를 DEVICE로 내보내고 \n",
    "            # 모델의 예측값(output)를 받아옴\n",
    "            \n",
    "            test_loss += F.cross_entropy(output,target, reduction='sum').item()\n",
    "            # 모든 테스트 오차 더하기\n",
    "            \n",
    "            \n",
    "            ## 맞힌 개수 계산 \n",
    "            pred = output.max(1, keepdim = True )[1]\n",
    "            # 가장 큰 값을 가진 클래스가 모델의 예측\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # 예측과 정답을 비교하여 일치할 경우 correct에 1을 더한다.\n",
    "            # view_as()는 target 텐서를 pred의 모양대로 정렬 \n",
    "            # eq()는 값이 일치하면 1  불일치하면 0\n",
    "            # eq()를 지나고 나면 배치 크기의 배열이 0이나 1로 채워짐\n",
    "            # sum() 함수로 배열 내의 모든 값을 더하면\n",
    "            # 모델이 정답을 맞춘 수를 구할 수 있다. \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset) # 오차 수 / 전체 테스트셋 수\n",
    "    test_accuracy = 100. *correct / len (test_loader.dataset) \n",
    "    #정확도= 정답을 맞춘 수 / 전체 테스트 셋 수  *100\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'trianing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-01bb6ccd8f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# 이폭마다 학습과 테스트셋을 이용해 검증을 반복하고 결과를 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-cb798f5fa8d0>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# 반복 시마다 기울기를 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 학습 데이터에 대한 모델의 예측값을 받아온다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-8a192acdbf9b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#드롭아웃 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         x = F.dropout(x, training = self.trianing,\n\u001b[0m\u001b[0;32m     18\u001b[0m                      p = self.dropout_p)\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fc2()를 거쳐 relu( )활성화 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    574\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 576\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Net' object has no attribute 'trianing'"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # 이폭마다 학습과 테스트셋을 이용해 검증을 반복하고 결과를 출력 \n",
    "    train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "          .format(epoch,test_loss,test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
