{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy #수치해석용 라이브러리 \n",
    "from sklearn.datasets import make_blobs #머신러닝용 라이브러리 \n",
    "import matplotlib.pyplot as plt #학습데이터의 분포와 패턴 시각화에 ㅇ용 \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋 생성 \n",
    "\n",
    "n_dim = 2\n",
    "\n",
    "# make_blobs() 함수는 레이블 데이터를 만든다.\n",
    "# 레이블 데이터는 각 데이터 한 점 한 점이 몇 번째 클러스터에 속해 있는지 알려주는 인덱스\n",
    "# 이번 예제에서는 4개의 클러스터를 만든다. \n",
    "# x_train과 x_test 속 모든 데이터는 0,1,2,3 으로 인덱싱이 되었다. \n",
    "\n",
    "x_train, y_train = make_blobs(n_samples = 80,n_features = n_dim,\n",
    "                              centers=[[1,1],[-1,-1],[1,-1],[-1,1]],\n",
    "                              shuffle=True,cluster_std =0.3\n",
    "                             )\n",
    "# 학습 데이터에 80개의 2차원 벡터 데이터셋을 담는다.\n",
    "# 셔플 = ture\n",
    "# cluster_std =0.3 -> 표준편차 가 0.3\n",
    "\n",
    "x_test,y_test = make_blobs(n_samples = 20,n_features = n_dim,\n",
    "                           centers=[[1,1],[-1,-1],[1,-1],[-1,1]],\n",
    "                           shuffle=True,cluster_std =0.3\n",
    "                        )\n",
    "# 평가 데이터에 20개의 2차원 벡터 데이터셋을 담는다.\n",
    "\n",
    "\n",
    "# 4개의 레이블을 2개로 합쳐본다. \n",
    "def label_map(y_,from_,to_):\n",
    "    # 0,1 번 레이블 데이터는 전부 0번 레이블 \n",
    "    # 2,3 번레이블 데이터는 전부 1번 레이블\n",
    "    y = numpy.copy(y_)\n",
    "    for f in from_:\n",
    "        y[y_ == f] = to_\n",
    "    return y\n",
    "\n",
    "y_train = label_map(y_train,[0,1],0)\n",
    "y_train = label_map(y_train,[2,3],1)\n",
    "y_test = label_map(y_test,[0,1],0)\n",
    "y_test = label_map(y_test,[2,3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 생성된 데이터셋 시각화 \n",
    "def vis_data(x,y = None, c ='r'):\n",
    "    if y is None:\n",
    "        y = [None]*len(x)\n",
    "    for x_,y_ in zip(x,y):\n",
    "        if y_ is None:\n",
    "            plt.plot(x_[0],x_[1],'*',markerfacecolor='none',markeredgecolor = c)\n",
    "            # markerfacecolor='none' 마커 내부 색은 nono\n",
    "            #,markeredgecolor = c ='r'마커 윤곽선색은 빨강색 \n",
    "        else:\n",
    "            plt.plot(x_[0],x_[1],c+'o' if y_ == 0 else c+'+')\n",
    "            #레이블이 0인 데이터는 점으로 (c+'o') \n",
    "            #1인 데이터는 +으로 표현 (c+'+')\n",
    "               \n",
    "plt.figure() \n",
    "vis_data(x_train,y_train,c='r') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋을 파이토치 텐서로 변환 \n",
    "x_train = torch.FloatTensor(x_train) \n",
    "x_test = torch.FloatTensor(x_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신경망 모델 정의\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    # 신경망은 보통 다음과 같이 신경망 모듈을 상속받는 파이썬 클래스로 정의\n",
    "    def __init__ (self, input_size, hidden_size): \n",
    "        super(NeuralNet, self).__init__()\n",
    "        #NeuralNet 크래스를 파이토치의 nn.Module 크래스의 속성들로 초기화 \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.linear_1 = torch.nn.Linear(self.input_size,self.hidden_size)\n",
    "        self.linear_2 = torch.nn.Linear(self.hidden_size,1)\n",
    "        #활성화함수로서 relu() 와 sigmoid ()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    #init() 함수에서 정의한 동작들을 차례대로 시행하는 forward 구현 \n",
    "    def forward(self, input_tensor):\n",
    "        linear1 = self.linear_1(input_tensor)\n",
    "        relu = self.relu(linear1)\n",
    "        # relu는 입력값이 0보다 작으면 0을 0보다 크면 그대로 출력 \n",
    "        linear2 = self.linear_2(relu)\n",
    "        output = self.sigmoid(linear2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 신경망 모델 생성 및 관련 변수와 알고리즘 정의 \n",
    "model = NeuralNet(2,5)\n",
    "#input_size : 2\n",
    "#hidden_size : 5\n",
    "learning_rate=0.03\n",
    "criterion=torch.nn.BCELoss()\n",
    "# 이차교차엔트로피 오차 함수를 사용 \n",
    "epochs = 2000\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "# 최적화 알고리즘으로 확률적 경사하강법 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training, test loss is0.692142128944397\n"
     ]
    }
   ],
   "source": [
    "## 학습 전 모델 성능 평가 \n",
    "model.eval()\n",
    "test_loss_before = criterion(model(x_test).squeeze(),y_test)\n",
    "#squeeze 함수로 차원을 맞춘다. \n",
    "print('Before Training, test loss is{}'.format(test_loss_before.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.6917605996131897)\n",
      "Train loss at 2000 is 0.6912246942520142)\n",
      "Train loss at 2000 is 0.6906763911247253)\n",
      "Train loss at 2000 is 0.6901295781135559)\n",
      "Train loss at 2000 is 0.6895847916603088)\n",
      "Train loss at 2000 is 0.689041256904602)\n",
      "Train loss at 2000 is 0.6884990334510803)\n",
      "Train loss at 2000 is 0.6879564523696899)\n",
      "Train loss at 2000 is 0.6874123811721802)\n",
      "Train loss at 2000 is 0.6868693828582764)\n",
      "Train loss at 2000 is 0.6863282322883606)\n",
      "Train loss at 2000 is 0.685790479183197)\n",
      "Train loss at 2000 is 0.6852535009384155)\n",
      "Train loss at 2000 is 0.684717059135437)\n",
      "Train loss at 2000 is 0.6841815114021301)\n",
      "Train loss at 2000 is 0.6836470365524292)\n",
      "Train loss at 2000 is 0.6831129193305969)\n",
      "Train loss at 2000 is 0.6825787425041199)\n",
      "Train loss at 2000 is 0.6820441484451294)\n",
      "Train loss at 2000 is 0.6815086603164673)\n",
      "Train loss at 2000 is 0.6809692978858948)\n",
      "Train loss at 2000 is 0.6804224848747253)\n",
      "Train loss at 2000 is 0.67987459897995)\n",
      "Train loss at 2000 is 0.6793261170387268)\n",
      "Train loss at 2000 is 0.678776741027832)\n",
      "Train loss at 2000 is 0.6782267689704895)\n",
      "Train loss at 2000 is 0.6776763200759888)\n",
      "Train loss at 2000 is 0.6771240234375)\n",
      "Train loss at 2000 is 0.6765701770782471)\n",
      "Train loss at 2000 is 0.6760151386260986)\n",
      "Train loss at 2000 is 0.6754589080810547)\n",
      "Train loss at 2000 is 0.6749016642570496)\n",
      "Train loss at 2000 is 0.6743448972702026)\n",
      "Train loss at 2000 is 0.6737868785858154)\n",
      "Train loss at 2000 is 0.6732267737388611)\n",
      "Train loss at 2000 is 0.6726627349853516)\n",
      "Train loss at 2000 is 0.6720969676971436)\n",
      "Train loss at 2000 is 0.6715292930603027)\n",
      "Train loss at 2000 is 0.6709596514701843)\n",
      "Train loss at 2000 is 0.6703879237174988)\n",
      "Train loss at 2000 is 0.66981440782547)\n",
      "Train loss at 2000 is 0.6692419052124023)\n",
      "Train loss at 2000 is 0.6686782836914062)\n",
      "Train loss at 2000 is 0.6681124567985535)\n",
      "Train loss at 2000 is 0.6675420999526978)\n",
      "Train loss at 2000 is 0.6669765114784241)\n",
      "Train loss at 2000 is 0.6664082407951355)\n",
      "Train loss at 2000 is 0.6658374667167664)\n",
      "Train loss at 2000 is 0.6652639508247375)\n",
      "Train loss at 2000 is 0.6646876335144043)\n",
      "Train loss at 2000 is 0.6641085147857666)\n",
      "Train loss at 2000 is 0.6635265350341797)\n",
      "Train loss at 2000 is 0.6629412770271301)\n",
      "Train loss at 2000 is 0.6623511910438538)\n",
      "Train loss at 2000 is 0.6617566347122192)\n",
      "Train loss at 2000 is 0.6611587405204773)\n",
      "Train loss at 2000 is 0.660557746887207)\n",
      "Train loss at 2000 is 0.6599542498588562)\n",
      "Train loss at 2000 is 0.6593495011329651)\n",
      "Train loss at 2000 is 0.6587428450584412)\n",
      "Train loss at 2000 is 0.658136248588562)\n",
      "Train loss at 2000 is 0.6575264930725098)\n",
      "Train loss at 2000 is 0.6569122076034546)\n",
      "Train loss at 2000 is 0.6562901735305786)\n",
      "Train loss at 2000 is 0.655664324760437)\n",
      "Train loss at 2000 is 0.6550346612930298)\n",
      "Train loss at 2000 is 0.6544015407562256)\n",
      "Train loss at 2000 is 0.6537638902664185)\n",
      "Train loss at 2000 is 0.6531111598014832)\n",
      "Train loss at 2000 is 0.652456521987915)\n",
      "Train loss at 2000 is 0.6517977714538574)\n",
      "Train loss at 2000 is 0.651138186454773)\n",
      "Train loss at 2000 is 0.6504753232002258)\n",
      "Train loss at 2000 is 0.6498104333877563)\n",
      "Train loss at 2000 is 0.6491438746452332)\n",
      "Train loss at 2000 is 0.6484914422035217)\n",
      "Train loss at 2000 is 0.6478398442268372)\n",
      "Train loss at 2000 is 0.6471869945526123)\n",
      "Train loss at 2000 is 0.6465390920639038)\n",
      "Train loss at 2000 is 0.6458913683891296)\n",
      "Train loss at 2000 is 0.6452440023422241)\n",
      "Train loss at 2000 is 0.6445940136909485)\n",
      "Train loss at 2000 is 0.6439398527145386)\n",
      "Train loss at 2000 is 0.6432819962501526)\n",
      "Train loss at 2000 is 0.6426213979721069)\n",
      "Train loss at 2000 is 0.6419559717178345)\n",
      "Train loss at 2000 is 0.641288697719574)\n",
      "Train loss at 2000 is 0.6406221985816956)\n",
      "Train loss at 2000 is 0.6399515867233276)\n",
      "Train loss at 2000 is 0.6392762660980225)\n",
      "Train loss at 2000 is 0.6385965347290039)\n",
      "Train loss at 2000 is 0.6379120349884033)\n",
      "Train loss at 2000 is 0.6372219920158386)\n",
      "Train loss at 2000 is 0.6365218758583069)\n",
      "Train loss at 2000 is 0.6358076333999634)\n",
      "Train loss at 2000 is 0.6350888013839722)\n",
      "Train loss at 2000 is 0.6343649625778198)\n",
      "Train loss at 2000 is 0.6336362957954407)\n",
      "Train loss at 2000 is 0.6329026222229004)\n",
      "Train loss at 2000 is 0.6321642398834229)\n",
      "Train loss at 2000 is 0.6314206719398499)\n",
      "Train loss at 2000 is 0.6306723356246948)\n",
      "Train loss at 2000 is 0.6299188733100891)\n",
      "Train loss at 2000 is 0.6291605234146118)\n",
      "Train loss at 2000 is 0.6283969879150391)\n",
      "Train loss at 2000 is 0.6276292204856873)\n",
      "Train loss at 2000 is 0.626867949962616)\n",
      "Train loss at 2000 is 0.6261018514633179)\n",
      "Train loss at 2000 is 0.6253305673599243)\n",
      "Train loss at 2000 is 0.6245543956756592)\n",
      "Train loss at 2000 is 0.6237727403640747)\n",
      "Train loss at 2000 is 0.6229861974716187)\n",
      "Train loss at 2000 is 0.6221946477890015)\n",
      "Train loss at 2000 is 0.621397852897644)\n",
      "Train loss at 2000 is 0.6205958724021912)\n",
      "Train loss at 2000 is 0.6197891235351562)\n",
      "Train loss at 2000 is 0.6189773678779602)\n",
      "Train loss at 2000 is 0.6181607246398926)\n",
      "Train loss at 2000 is 0.6173394322395325)\n",
      "Train loss at 2000 is 0.6165129542350769)\n",
      "Train loss at 2000 is 0.6156812310218811)\n",
      "Train loss at 2000 is 0.6148440837860107)\n",
      "Train loss at 2000 is 0.6140018701553345)\n",
      "Train loss at 2000 is 0.6131540536880493)\n",
      "Train loss at 2000 is 0.6123011112213135)\n",
      "Train loss at 2000 is 0.6114424467086792)\n",
      "Train loss at 2000 is 0.6105788946151733)\n",
      "Train loss at 2000 is 0.6097097992897034)\n",
      "Train loss at 2000 is 0.6088353991508484)\n",
      "Train loss at 2000 is 0.607962965965271)\n",
      "Train loss at 2000 is 0.6071103811264038)\n",
      "Train loss at 2000 is 0.606261670589447)\n",
      "Train loss at 2000 is 0.6054081916809082)\n",
      "Train loss at 2000 is 0.6045501828193665)\n",
      "Train loss at 2000 is 0.6036932468414307)\n",
      "Train loss at 2000 is 0.6028317213058472)\n",
      "Train loss at 2000 is 0.6019650101661682)\n",
      "Train loss at 2000 is 0.6010932922363281)\n",
      "Train loss at 2000 is 0.6002165079116821)\n",
      "Train loss at 2000 is 0.5993348956108093)\n",
      "Train loss at 2000 is 0.5984484553337097)\n",
      "Train loss at 2000 is 0.5975578427314758)\n",
      "Train loss at 2000 is 0.5966633558273315)\n",
      "Train loss at 2000 is 0.5957626104354858)\n",
      "Train loss at 2000 is 0.594861626625061)\n",
      "Train loss at 2000 is 0.5939623713493347)\n",
      "Train loss at 2000 is 0.59305739402771)\n",
      "Train loss at 2000 is 0.5921463966369629)\n",
      "Train loss at 2000 is 0.591230571269989)\n",
      "Train loss at 2000 is 0.5903109312057495)\n",
      "Train loss at 2000 is 0.5893872976303101)\n",
      "Train loss at 2000 is 0.5884591341018677)\n",
      "Train loss at 2000 is 0.5875262022018433)\n",
      "Train loss at 2000 is 0.5865886807441711)\n",
      "Train loss at 2000 is 0.5856470465660095)\n",
      "Train loss at 2000 is 0.5847128629684448)\n",
      "Train loss at 2000 is 0.5837722420692444)\n",
      "Train loss at 2000 is 0.5828365683555603)\n",
      "Train loss at 2000 is 0.5818983912467957)\n",
      "Train loss at 2000 is 0.5809653997421265)\n",
      "Train loss at 2000 is 0.5800315141677856)\n",
      "Train loss at 2000 is 0.5790930986404419)\n",
      "Train loss at 2000 is 0.5781506896018982)\n",
      "Train loss at 2000 is 0.5772038102149963)\n",
      "Train loss at 2000 is 0.5762529969215393)\n",
      "Train loss at 2000 is 0.5752977132797241)\n",
      "Train loss at 2000 is 0.5743460059165955)\n",
      "Train loss at 2000 is 0.5734041929244995)\n",
      "Train loss at 2000 is 0.5724582076072693)\n",
      "Train loss at 2000 is 0.5715078115463257)\n",
      "Train loss at 2000 is 0.5705504417419434)\n",
      "Train loss at 2000 is 0.5695894360542297)\n",
      "Train loss at 2000 is 0.5686243176460266)\n",
      "Train loss at 2000 is 0.5676552653312683)\n",
      "Train loss at 2000 is 0.5666822195053101)\n",
      "Train loss at 2000 is 0.5657055974006653)\n",
      "Train loss at 2000 is 0.5647251605987549)\n",
      "Train loss at 2000 is 0.5637418031692505)\n",
      "Train loss at 2000 is 0.5627632737159729)\n",
      "Train loss at 2000 is 0.561781108379364)\n",
      "Train loss at 2000 is 0.560795247554779)\n",
      "Train loss at 2000 is 0.5598064661026001)\n",
      "Train loss at 2000 is 0.5588141083717346)\n",
      "Train loss at 2000 is 0.5578182935714722)\n",
      "Train loss at 2000 is 0.5568190813064575)\n",
      "Train loss at 2000 is 0.5558165311813354)\n",
      "Train loss at 2000 is 0.5548105835914612)\n",
      "Train loss at 2000 is 0.5538011789321899)\n",
      "Train loss at 2000 is 0.5527883768081665)\n",
      "Train loss at 2000 is 0.5517725348472595)\n",
      "Train loss at 2000 is 0.5507627129554749)\n",
      "Train loss at 2000 is 0.5497722029685974)\n",
      "Train loss at 2000 is 0.5487788319587708)\n",
      "Train loss at 2000 is 0.547782301902771)\n",
      "Train loss at 2000 is 0.5467798709869385)\n",
      "Train loss at 2000 is 0.5457733869552612)\n",
      "Train loss at 2000 is 0.5447641015052795)\n",
      "Train loss at 2000 is 0.543753445148468)\n",
      "Train loss at 2000 is 0.5427564382553101)\n",
      "Train loss at 2000 is 0.5417643785476685)\n",
      "Train loss at 2000 is 0.5407693386077881)\n",
      "Train loss at 2000 is 0.539772629737854)\n",
      "Train loss at 2000 is 0.5387728810310364)\n",
      "Train loss at 2000 is 0.5377711057662964)\n",
      "Train loss at 2000 is 0.5367666482925415)\n",
      "Train loss at 2000 is 0.5357601642608643)\n",
      "Train loss at 2000 is 0.5347511768341064)\n",
      "Train loss at 2000 is 0.5337426066398621)\n",
      "Train loss at 2000 is 0.5327361822128296)\n",
      "Train loss at 2000 is 0.5317273736000061)\n",
      "Train loss at 2000 is 0.5307162404060364)\n",
      "Train loss at 2000 is 0.5297031998634338)\n",
      "Train loss at 2000 is 0.5286877155303955)\n",
      "Train loss at 2000 is 0.527671217918396)\n",
      "Train loss at 2000 is 0.5266539454460144)\n",
      "Train loss at 2000 is 0.5256344079971313)\n",
      "Train loss at 2000 is 0.5246127843856812)\n",
      "Train loss at 2000 is 0.5235892534255981)\n",
      "Train loss at 2000 is 0.5225635170936584)\n",
      "Train loss at 2000 is 0.5215356349945068)\n",
      "Train loss at 2000 is 0.5205059051513672)\n",
      "Train loss at 2000 is 0.5194741487503052)\n",
      "Train loss at 2000 is 0.5184406638145447)\n",
      "Train loss at 2000 is 0.5174049139022827)\n",
      "Train loss at 2000 is 0.5163676142692566)\n",
      "Train loss at 2000 is 0.5153284072875977)\n",
      "Train loss at 2000 is 0.5142878293991089)\n",
      "Train loss at 2000 is 0.5132464170455933)\n",
      "Train loss at 2000 is 0.5122041702270508)\n",
      "Train loss at 2000 is 0.5111591815948486)\n",
      "Train loss at 2000 is 0.5101145505905151)\n",
      "Train loss at 2000 is 0.5090757608413696)\n",
      "Train loss at 2000 is 0.5080447196960449)\n",
      "Train loss at 2000 is 0.5070121884346008)\n",
      "Train loss at 2000 is 0.5059786438941956)\n",
      "Train loss at 2000 is 0.5049439668655396)\n",
      "Train loss at 2000 is 0.5039080381393433)\n",
      "Train loss at 2000 is 0.5028711557388306)\n",
      "Train loss at 2000 is 0.5018330216407776)\n",
      "Train loss at 2000 is 0.5007933378219604)\n",
      "Train loss at 2000 is 0.49974989891052246)\n",
      "Train loss at 2000 is 0.4987057149410248)\n",
      "Train loss at 2000 is 0.49766048789024353)\n",
      "Train loss at 2000 is 0.49661439657211304)\n",
      "Train loss at 2000 is 0.4955684244632721)\n",
      "Train loss at 2000 is 0.49452272057533264)\n",
      "Train loss at 2000 is 0.49348315596580505)\n",
      "Train loss at 2000 is 0.4924428462982178)\n",
      "Train loss at 2000 is 0.49140357971191406)\n",
      "Train loss at 2000 is 0.490369975566864)\n",
      "Train loss at 2000 is 0.48933619260787964)\n",
      "Train loss at 2000 is 0.48830223083496094)\n",
      "Train loss at 2000 is 0.4872681498527527)\n",
      "Train loss at 2000 is 0.4862334728240967)\n",
      "Train loss at 2000 is 0.4851984977722168)\n",
      "Train loss at 2000 is 0.4841632843017578)\n",
      "Train loss at 2000 is 0.48312777280807495)\n",
      "Train loss at 2000 is 0.48209771513938904)\n",
      "Train loss at 2000 is 0.48106759786605835)\n",
      "Train loss at 2000 is 0.480037122964859)\n",
      "Train loss at 2000 is 0.47900691628456116)\n",
      "Train loss at 2000 is 0.477975070476532)\n",
      "Train loss at 2000 is 0.4769418239593506)\n",
      "Train loss at 2000 is 0.4759083688259125)\n",
      "Train loss at 2000 is 0.4748750627040863)\n",
      "Train loss at 2000 is 0.4738423824310303)\n",
      "Train loss at 2000 is 0.4728114604949951)\n",
      "Train loss at 2000 is 0.47179436683654785)\n",
      "Train loss at 2000 is 0.4707774221897125)\n",
      "Train loss at 2000 is 0.46976032853126526)\n",
      "Train loss at 2000 is 0.46874088048934937)\n",
      "Train loss at 2000 is 0.4677217900753021)\n",
      "Train loss at 2000 is 0.4667030870914459)\n",
      "Train loss at 2000 is 0.46568456292152405)\n",
      "Train loss at 2000 is 0.4646662175655365)\n",
      "Train loss at 2000 is 0.4636484682559967)\n",
      "Train loss at 2000 is 0.46263188123703003)\n",
      "Train loss at 2000 is 0.4616156220436096)\n",
      "Train loss at 2000 is 0.4605996608734131)\n",
      "Train loss at 2000 is 0.4595840871334076)\n",
      "Train loss at 2000 is 0.4585689902305603)\n",
      "Train loss at 2000 is 0.45755428075790405)\n",
      "Train loss at 2000 is 0.456540048122406)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.45554009079933167)\n",
      "Train loss at 2000 is 0.4545406401157379)\n",
      "Train loss at 2000 is 0.45354214310646057)\n",
      "Train loss at 2000 is 0.4525441527366638)\n",
      "Train loss at 2000 is 0.45155295729637146)\n",
      "Train loss at 2000 is 0.4505624771118164)\n",
      "Train loss at 2000 is 0.4495725631713867)\n",
      "Train loss at 2000 is 0.44858360290527344)\n",
      "Train loss at 2000 is 0.44759517908096313)\n",
      "Train loss at 2000 is 0.446607768535614)\n",
      "Train loss at 2000 is 0.4456210136413574)\n",
      "Train loss at 2000 is 0.4446350038051605)\n",
      "Train loss at 2000 is 0.4436473846435547)\n",
      "Train loss at 2000 is 0.4426611363887787)\n",
      "Train loss at 2000 is 0.4416786730289459)\n",
      "Train loss at 2000 is 0.4406971037387848)\n",
      "Train loss at 2000 is 0.4397163987159729)\n",
      "Train loss at 2000 is 0.4387362599372864)\n",
      "Train loss at 2000 is 0.4377569556236267)\n",
      "Train loss at 2000 is 0.43678075075149536)\n",
      "Train loss at 2000 is 0.4358157515525818)\n",
      "Train loss at 2000 is 0.4348514974117279)\n",
      "Train loss at 2000 is 0.43388810753822327)\n",
      "Train loss at 2000 is 0.4329254627227783)\n",
      "Train loss at 2000 is 0.4319635331630707)\n",
      "Train loss at 2000 is 0.43100252747535706)\n",
      "Train loss at 2000 is 0.4300423562526703)\n",
      "Train loss at 2000 is 0.42908310890197754)\n",
      "Train loss at 2000 is 0.42812466621398926)\n",
      "Train loss at 2000 is 0.4271671175956726)\n",
      "Train loss at 2000 is 0.4262104630470276)\n",
      "Train loss at 2000 is 0.425254762172699)\n",
      "Train loss at 2000 is 0.424299955368042)\n",
      "Train loss at 2000 is 0.4233459532260895)\n",
      "Train loss at 2000 is 0.42239299416542053)\n",
      "Train loss at 2000 is 0.4214410185813904)\n",
      "Train loss at 2000 is 0.4204898774623871)\n",
      "Train loss at 2000 is 0.41953960061073303)\n",
      "Train loss at 2000 is 0.41859111189842224)\n",
      "Train loss at 2000 is 0.41764798760414124)\n",
      "Train loss at 2000 is 0.416706383228302)\n",
      "Train loss at 2000 is 0.4157656729221344)\n",
      "Train loss at 2000 is 0.41482624411582947)\n",
      "Train loss at 2000 is 0.4138879179954529)\n",
      "Train loss at 2000 is 0.41295093297958374)\n",
      "Train loss at 2000 is 0.41201552748680115)\n",
      "Train loss at 2000 is 0.4110811650753021)\n",
      "Train loss at 2000 is 0.4101477563381195)\n",
      "Train loss at 2000 is 0.40921610593795776)\n",
      "Train loss at 2000 is 0.4082857668399811)\n",
      "Train loss at 2000 is 0.40735626220703125)\n",
      "Train loss at 2000 is 0.40642914175987244)\n",
      "Train loss at 2000 is 0.4055022597312927)\n",
      "Train loss at 2000 is 0.4045785069465637)\n",
      "Train loss at 2000 is 0.40365535020828247)\n",
      "Train loss at 2000 is 0.4027339816093445)\n",
      "Train loss at 2000 is 0.4018142819404602)\n",
      "Train loss at 2000 is 0.400895893573761)\n",
      "Train loss at 2000 is 0.3999805450439453)\n",
      "Train loss at 2000 is 0.3990672528743744)\n",
      "Train loss at 2000 is 0.39815884828567505)\n",
      "Train loss at 2000 is 0.39725184440612793)\n",
      "Train loss at 2000 is 0.3963470160961151)\n",
      "Train loss at 2000 is 0.3954433798789978)\n",
      "Train loss at 2000 is 0.39454206824302673)\n",
      "Train loss at 2000 is 0.3936423063278198)\n",
      "Train loss at 2000 is 0.3927440941333771)\n",
      "Train loss at 2000 is 0.39184802770614624)\n",
      "Train loss at 2000 is 0.39095327258110046)\n",
      "Train loss at 2000 is 0.39006075263023376)\n",
      "Train loss at 2000 is 0.3891701102256775)\n",
      "Train loss at 2000 is 0.3882807195186615)\n",
      "Train loss at 2000 is 0.3873937129974365)\n",
      "Train loss at 2000 is 0.38650840520858765)\n",
      "Train loss at 2000 is 0.38562458753585815)\n",
      "Train loss at 2000 is 0.3847429156303406)\n",
      "Train loss at 2000 is 0.38386303186416626)\n",
      "Train loss at 2000 is 0.38298508524894714)\n",
      "Train loss at 2000 is 0.38210904598236084)\n",
      "Train loss at 2000 is 0.3812345862388611)\n",
      "Train loss at 2000 is 0.38036197423934937)\n",
      "Train loss at 2000 is 0.37949150800704956)\n",
      "Train loss at 2000 is 0.3786228597164154)\n",
      "Train loss at 2000 is 0.37775617837905884)\n",
      "Train loss at 2000 is 0.376891165971756)\n",
      "Train loss at 2000 is 0.3760283589363098)\n",
      "Train loss at 2000 is 0.37516725063323975)\n",
      "Train loss at 2000 is 0.37430793046951294)\n",
      "Train loss at 2000 is 0.3734505772590637)\n",
      "Train loss at 2000 is 0.3725953698158264)\n",
      "Train loss at 2000 is 0.3717418611049652)\n",
      "Train loss at 2000 is 0.37089022994041443)\n",
      "Train loss at 2000 is 0.3700405955314636)\n",
      "Train loss at 2000 is 0.369192898273468)\n",
      "Train loss at 2000 is 0.36834707856178284)\n",
      "Train loss at 2000 is 0.3675038516521454)\n",
      "Train loss at 2000 is 0.3666694462299347)\n",
      "Train loss at 2000 is 0.36583706736564636)\n",
      "Train loss at 2000 is 0.3650065064430237)\n",
      "Train loss at 2000 is 0.3641778826713562)\n",
      "Train loss at 2000 is 0.3633509576320648)\n",
      "Train loss at 2000 is 0.3625260293483734)\n",
      "Train loss at 2000 is 0.36170294880867004)\n",
      "Train loss at 2000 is 0.3608817458152771)\n",
      "Train loss at 2000 is 0.3600623905658722)\n",
      "Train loss at 2000 is 0.3592446446418762)\n",
      "Train loss at 2000 is 0.35842961072921753)\n",
      "Train loss at 2000 is 0.35761988162994385)\n",
      "Train loss at 2000 is 0.3568119406700134)\n",
      "Train loss at 2000 is 0.35600608587265015)\n",
      "Train loss at 2000 is 0.35520225763320923)\n",
      "Train loss at 2000 is 0.35440054535865784)\n",
      "Train loss at 2000 is 0.35360080003738403)\n",
      "Train loss at 2000 is 0.3528030514717102)\n",
      "Train loss at 2000 is 0.35200729966163635)\n",
      "Train loss at 2000 is 0.35121357440948486)\n",
      "Train loss at 2000 is 0.35042187571525574)\n",
      "Train loss at 2000 is 0.3496320843696594)\n",
      "Train loss at 2000 is 0.3488444685935974)\n",
      "Train loss at 2000 is 0.3480587899684906)\n",
      "Train loss at 2000 is 0.3472749888896942)\n",
      "Train loss at 2000 is 0.3464932143688202)\n",
      "Train loss at 2000 is 0.3457135260105133)\n",
      "Train loss at 2000 is 0.3449357748031616)\n",
      "Train loss at 2000 is 0.3441600203514099)\n",
      "Train loss at 2000 is 0.343386173248291)\n",
      "Train loss at 2000 is 0.34261441230773926)\n",
      "Train loss at 2000 is 0.3418445885181427)\n",
      "Train loss at 2000 is 0.3410767614841461)\n",
      "Train loss at 2000 is 0.3403109312057495)\n",
      "Train loss at 2000 is 0.3395468592643738)\n",
      "Train loss at 2000 is 0.3387848138809204)\n",
      "Train loss at 2000 is 0.33802488446235657)\n",
      "Train loss at 2000 is 0.3372666835784912)\n",
      "Train loss at 2000 is 0.3365105986595154)\n",
      "Train loss at 2000 is 0.33575639128685)\n",
      "Train loss at 2000 is 0.3350040316581726)\n",
      "Train loss at 2000 is 0.3342536985874176)\n",
      "Train loss at 2000 is 0.3335053324699402)\n",
      "Train loss at 2000 is 0.33275872468948364)\n",
      "Train loss at 2000 is 0.332014262676239)\n",
      "Train loss at 2000 is 0.33127158880233765)\n",
      "Train loss at 2000 is 0.33053073287010193)\n",
      "Train loss at 2000 is 0.3297919034957886)\n",
      "Train loss at 2000 is 0.32905498147010803)\n",
      "Train loss at 2000 is 0.3283199071884155)\n",
      "Train loss at 2000 is 0.3275867998600006)\n",
      "Train loss at 2000 is 0.32685545086860657)\n",
      "Train loss at 2000 is 0.3261260390281677)\n",
      "Train loss at 2000 is 0.3253985047340393)\n",
      "Train loss at 2000 is 0.32467472553253174)\n",
      "Train loss at 2000 is 0.32395702600479126)\n",
      "Train loss at 2000 is 0.32324138283729553)\n",
      "Train loss at 2000 is 0.3225279748439789)\n",
      "Train loss at 2000 is 0.32181674242019653)\n",
      "Train loss at 2000 is 0.3211074471473694)\n",
      "Train loss at 2000 is 0.3204003572463989)\n",
      "Train loss at 2000 is 0.31969547271728516)\n",
      "Train loss at 2000 is 0.31899258494377136)\n",
      "Train loss at 2000 is 0.31829187273979187)\n",
      "Train loss at 2000 is 0.3175932765007019)\n",
      "Train loss at 2000 is 0.31689658761024475)\n",
      "Train loss at 2000 is 0.31620219349861145)\n",
      "Train loss at 2000 is 0.31550973653793335)\n",
      "Train loss at 2000 is 0.3148193657398224)\n",
      "Train loss at 2000 is 0.31413108110427856)\n",
      "Train loss at 2000 is 0.31344476342201233)\n",
      "Train loss at 2000 is 0.31276053190231323)\n",
      "Train loss at 2000 is 0.3120783865451813)\n",
      "Train loss at 2000 is 0.3113982081413269)\n",
      "Train loss at 2000 is 0.3107200562953949)\n",
      "Train loss at 2000 is 0.3100438416004181)\n",
      "Train loss at 2000 is 0.30936965346336365)\n",
      "Train loss at 2000 is 0.30869752168655396)\n",
      "Train loss at 2000 is 0.30802732706069946)\n",
      "Train loss at 2000 is 0.30735912919044495)\n",
      "Train loss at 2000 is 0.3066929280757904)\n",
      "Train loss at 2000 is 0.3060285151004791)\n",
      "Train loss at 2000 is 0.3053661286830902)\n",
      "Train loss at 2000 is 0.30470576882362366)\n",
      "Train loss at 2000 is 0.304047167301178)\n",
      "Train loss at 2000 is 0.3033907413482666)\n",
      "Train loss at 2000 is 0.3027361035346985)\n",
      "Train loss at 2000 is 0.3020833730697632)\n",
      "Train loss at 2000 is 0.3014325201511383)\n",
      "Train loss at 2000 is 0.300783634185791)\n",
      "Train loss at 2000 is 0.300136536359787)\n",
      "Train loss at 2000 is 0.29949134588241577)\n",
      "Train loss at 2000 is 0.2988480031490326)\n",
      "Train loss at 2000 is 0.29820650815963745)\n",
      "Train loss at 2000 is 0.29756686091423035)\n",
      "Train loss at 2000 is 0.29692915081977844)\n",
      "Train loss at 2000 is 0.2962931990623474)\n",
      "Train loss at 2000 is 0.2956591248512268)\n",
      "Train loss at 2000 is 0.29502683877944946)\n",
      "Train loss at 2000 is 0.2943962812423706)\n",
      "Train loss at 2000 is 0.29376763105392456)\n",
      "Train loss at 2000 is 0.2931406795978546)\n",
      "Train loss at 2000 is 0.29251569509506226)\n",
      "Train loss at 2000 is 0.2918923497200012)\n",
      "Train loss at 2000 is 0.29127076268196106)\n",
      "Train loss at 2000 is 0.2906508445739746)\n",
      "Train loss at 2000 is 0.29003292322158813)\n",
      "Train loss at 2000 is 0.2894166111946106)\n",
      "Train loss at 2000 is 0.28880199790000916)\n",
      "Train loss at 2000 is 0.28818923234939575)\n",
      "Train loss at 2000 is 0.2875782251358032)\n",
      "Train loss at 2000 is 0.28696876764297485)\n",
      "Train loss at 2000 is 0.2863611578941345)\n",
      "Train loss at 2000 is 0.2857550382614136)\n",
      "Train loss at 2000 is 0.285150945186615)\n",
      "Train loss at 2000 is 0.28454989194869995)\n",
      "Train loss at 2000 is 0.28395047783851624)\n",
      "Train loss at 2000 is 0.283352792263031)\n",
      "Train loss at 2000 is 0.2827567458152771)\n",
      "Train loss at 2000 is 0.28216230869293213)\n",
      "Train loss at 2000 is 0.28156954050064087)\n",
      "Train loss at 2000 is 0.28097859025001526)\n",
      "Train loss at 2000 is 0.2803891599178314)\n",
      "Train loss at 2000 is 0.27980130910873413)\n",
      "Train loss at 2000 is 0.2792152464389801)\n",
      "Train loss at 2000 is 0.2786306142807007)\n",
      "Train loss at 2000 is 0.2780476212501526)\n",
      "Train loss at 2000 is 0.2774663269519806)\n",
      "Train loss at 2000 is 0.276886522769928)\n",
      "Train loss at 2000 is 0.27630841732025146)\n",
      "Train loss at 2000 is 0.27573180198669434)\n",
      "Train loss at 2000 is 0.275156706571579)\n",
      "Train loss at 2000 is 0.2745833694934845)\n",
      "Train loss at 2000 is 0.274011492729187)\n",
      "Train loss at 2000 is 0.27344101667404175)\n",
      "Train loss at 2000 is 0.27287232875823975)\n",
      "Train loss at 2000 is 0.27230510115623474)\n",
      "Train loss at 2000 is 0.2717398703098297)\n",
      "Train loss at 2000 is 0.27117687463760376)\n",
      "Train loss at 2000 is 0.2706153392791748)\n",
      "Train loss at 2000 is 0.27005550265312195)\n",
      "Train loss at 2000 is 0.2694970369338989)\n",
      "Train loss at 2000 is 0.26894015073776245)\n",
      "Train loss at 2000 is 0.26838475465774536)\n",
      "Train loss at 2000 is 0.26783087849617004)\n",
      "Train loss at 2000 is 0.26727935671806335)\n",
      "Train loss at 2000 is 0.26672935485839844)\n",
      "Train loss at 2000 is 0.2661809027194977)\n",
      "Train loss at 2000 is 0.2656339108943939)\n",
      "Train loss at 2000 is 0.2650882601737976)\n",
      "Train loss at 2000 is 0.26454418897628784)\n",
      "Train loss at 2000 is 0.2640015482902527)\n",
      "Train loss at 2000 is 0.2634603679180145)\n",
      "Train loss at 2000 is 0.26292070746421814)\n",
      "Train loss at 2000 is 0.2623824179172516)\n",
      "Train loss at 2000 is 0.2618454098701477)\n",
      "Train loss at 2000 is 0.26130998134613037)\n",
      "Train loss at 2000 is 0.2607758939266205)\n",
      "Train loss at 2000 is 0.2602432370185852)\n",
      "Train loss at 2000 is 0.25971195101737976)\n",
      "Train loss at 2000 is 0.2591821253299713)\n",
      "Train loss at 2000 is 0.2586537003517151)\n",
      "Train loss at 2000 is 0.2581266760826111)\n",
      "Train loss at 2000 is 0.257600873708725)\n",
      "Train loss at 2000 is 0.257076621055603)\n",
      "Train loss at 2000 is 0.25655364990234375)\n",
      "Train loss at 2000 is 0.25603216886520386)\n",
      "Train loss at 2000 is 0.25551196932792664)\n",
      "Train loss at 2000 is 0.2549929916858673)\n",
      "Train loss at 2000 is 0.25447553396224976)\n",
      "Train loss at 2000 is 0.25396013259887695)\n",
      "Train loss at 2000 is 0.25344616174697876)\n",
      "Train loss at 2000 is 0.25293344259262085)\n",
      "Train loss at 2000 is 0.2524220943450928)\n",
      "Train loss at 2000 is 0.25191211700439453)\n",
      "Train loss at 2000 is 0.2514033913612366)\n",
      "Train loss at 2000 is 0.2508959472179413)\n",
      "Train loss at 2000 is 0.25038987398147583)\n",
      "Train loss at 2000 is 0.24988511204719543)\n",
      "Train loss at 2000 is 0.24938161671161652)\n",
      "Train loss at 2000 is 0.24887947738170624)\n",
      "Train loss at 2000 is 0.24837858974933624)\n",
      "Train loss at 2000 is 0.24787895381450653)\n",
      "Train loss at 2000 is 0.2473805695772171)\n",
      "Train loss at 2000 is 0.24688351154327393)\n",
      "Train loss at 2000 is 0.24638769030570984)\n",
      "Train loss at 2000 is 0.24589312076568604)\n",
      "Train loss at 2000 is 0.2453998625278473)\n",
      "Train loss at 2000 is 0.24490781128406525)\n",
      "Train loss at 2000 is 0.24441702663898468)\n",
      "Train loss at 2000 is 0.2439274787902832)\n",
      "Train loss at 2000 is 0.24343915283679962)\n",
      "Train loss at 2000 is 0.2429521381855011)\n",
      "Train loss at 2000 is 0.24246621131896973)\n",
      "Train loss at 2000 is 0.2419816553592682)\n",
      "Train loss at 2000 is 0.241498202085495)\n",
      "Train loss at 2000 is 0.24101607501506805)\n",
      "Train loss at 2000 is 0.24053499102592468)\n",
      "Train loss at 2000 is 0.24005524814128876)\n",
      "Train loss at 2000 is 0.2395767867565155)\n",
      "Train loss at 2000 is 0.23909933865070343)\n",
      "Train loss at 2000 is 0.2386232167482376)\n",
      "Train loss at 2000 is 0.23814861476421356)\n",
      "Train loss at 2000 is 0.237675741314888)\n",
      "Train loss at 2000 is 0.2372039258480072)\n",
      "Train loss at 2000 is 0.2367333471775055)\n",
      "Train loss at 2000 is 0.23626387119293213)\n",
      "Train loss at 2000 is 0.23579564690589905)\n",
      "Train loss at 2000 is 0.23532840609550476)\n",
      "Train loss at 2000 is 0.2348625212907791)\n",
      "Train loss at 2000 is 0.2343977689743042)\n",
      "Train loss at 2000 is 0.23393408954143524)\n",
      "Train loss at 2000 is 0.23347151279449463)\n",
      "Train loss at 2000 is 0.23301012814044952)\n",
      "Train loss at 2000 is 0.23254983127117157)\n",
      "Train loss at 2000 is 0.23209071159362793)\n",
      "Train loss at 2000 is 0.23163262009620667)\n",
      "Train loss at 2000 is 0.2311757355928421)\n",
      "Train loss at 2000 is 0.2307199239730835)\n",
      "Train loss at 2000 is 0.23026533424854279)\n",
      "Train loss at 2000 is 0.2298116683959961)\n",
      "Train loss at 2000 is 0.2293592393398285)\n",
      "Train loss at 2000 is 0.228907972574234)\n",
      "Train loss at 2000 is 0.22845759987831116)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.22800853848457336)\n",
      "Train loss at 2000 is 0.2275604009628296)\n",
      "Train loss at 2000 is 0.2271134853363037)\n",
      "Train loss at 2000 is 0.2266675978899002)\n",
      "Train loss at 2000 is 0.22622275352478027)\n",
      "Train loss at 2000 is 0.22577910125255585)\n",
      "Train loss at 2000 is 0.22533643245697021)\n",
      "Train loss at 2000 is 0.22489488124847412)\n",
      "Train loss at 2000 is 0.22445432841777802)\n",
      "Train loss at 2000 is 0.22401492297649384)\n",
      "Train loss at 2000 is 0.22357654571533203)\n",
      "Train loss at 2000 is 0.22313928604125977)\n",
      "Train loss at 2000 is 0.2227030247449875)\n",
      "Train loss at 2000 is 0.22226789593696594)\n",
      "Train loss at 2000 is 0.22183378040790558)\n",
      "Train loss at 2000 is 0.22140070796012878)\n",
      "Train loss at 2000 is 0.22096867859363556)\n",
      "Train loss at 2000 is 0.22053778171539307)\n",
      "Train loss at 2000 is 0.22010771930217743)\n",
      "Train loss at 2000 is 0.21967890858650208)\n",
      "Train loss at 2000 is 0.21925106644630432)\n",
      "Train loss at 2000 is 0.21882422268390656)\n",
      "Train loss at 2000 is 0.21839845180511475)\n",
      "Train loss at 2000 is 0.21797366440296173)\n",
      "Train loss at 2000 is 0.21754992008209229)\n",
      "Train loss at 2000 is 0.21713170409202576)\n",
      "Train loss at 2000 is 0.21671995520591736)\n",
      "Train loss at 2000 is 0.2163093388080597)\n",
      "Train loss at 2000 is 0.21590092778205872)\n",
      "Train loss at 2000 is 0.21549582481384277)\n",
      "Train loss at 2000 is 0.2150919884443283)\n",
      "Train loss at 2000 is 0.2146892547607422)\n",
      "Train loss at 2000 is 0.21428784728050232)\n",
      "Train loss at 2000 is 0.2138875424861908)\n",
      "Train loss at 2000 is 0.2134884148836136)\n",
      "Train loss at 2000 is 0.21309038996696472)\n",
      "Train loss at 2000 is 0.21269360184669495)\n",
      "Train loss at 2000 is 0.21229906380176544)\n",
      "Train loss at 2000 is 0.21191099286079407)\n",
      "Train loss at 2000 is 0.21152427792549133)\n",
      "Train loss at 2000 is 0.21113872528076172)\n",
      "Train loss at 2000 is 0.21075427532196045)\n",
      "Train loss at 2000 is 0.2103709876537323)\n",
      "Train loss at 2000 is 0.20998892188072205)\n",
      "Train loss at 2000 is 0.2096080482006073)\n",
      "Train loss at 2000 is 0.2092282474040985)\n",
      "Train loss at 2000 is 0.20884969830513)\n",
      "Train loss at 2000 is 0.20847217738628387)\n",
      "Train loss at 2000 is 0.20809590816497803)\n",
      "Train loss at 2000 is 0.20772060751914978)\n",
      "Train loss at 2000 is 0.20734703540802002)\n",
      "Train loss at 2000 is 0.20697586238384247)\n",
      "Train loss at 2000 is 0.20660603046417236)\n",
      "Train loss at 2000 is 0.2062370777130127)\n",
      "Train loss at 2000 is 0.20586928725242615)\n",
      "Train loss at 2000 is 0.20550265908241272)\n",
      "Train loss at 2000 is 0.20513701438903809)\n",
      "Train loss at 2000 is 0.20477254688739777)\n",
      "Train loss at 2000 is 0.20440904796123505)\n",
      "Train loss at 2000 is 0.2040465623140335)\n",
      "Train loss at 2000 is 0.2036851942539215)\n",
      "Train loss at 2000 is 0.2033248245716095)\n",
      "Train loss at 2000 is 0.20296558737754822)\n",
      "Train loss at 2000 is 0.20260730385780334)\n",
      "Train loss at 2000 is 0.20225009322166443)\n",
      "Train loss at 2000 is 0.2018938511610031)\n",
      "Train loss at 2000 is 0.20153871178627014)\n",
      "Train loss at 2000 is 0.20118451118469238)\n",
      "Train loss at 2000 is 0.20083138346672058)\n",
      "Train loss at 2000 is 0.2004791796207428)\n",
      "Train loss at 2000 is 0.2001279592514038)\n",
      "Train loss at 2000 is 0.19977787137031555)\n",
      "Train loss at 2000 is 0.19942858815193176)\n",
      "Train loss at 2000 is 0.19908039271831512)\n",
      "Train loss at 2000 is 0.19873318076133728)\n",
      "Train loss at 2000 is 0.19838689267635345)\n",
      "Train loss at 2000 is 0.19804155826568604)\n",
      "Train loss at 2000 is 0.19769719243049622)\n",
      "Train loss at 2000 is 0.197353795170784)\n",
      "Train loss at 2000 is 0.19701138138771057)\n",
      "Train loss at 2000 is 0.19666984677314758)\n",
      "Train loss at 2000 is 0.196329265832901)\n",
      "Train loss at 2000 is 0.1959897130727768)\n",
      "Train loss at 2000 is 0.19565096497535706)\n",
      "Train loss at 2000 is 0.1953132152557373)\n",
      "Train loss at 2000 is 0.19497637450695038)\n",
      "Train loss at 2000 is 0.19464044272899628)\n",
      "Train loss at 2000 is 0.19430537521839142)\n",
      "Train loss at 2000 is 0.19397132098674774)\n",
      "Train loss at 2000 is 0.19363808631896973)\n",
      "Train loss at 2000 is 0.19330576062202454)\n",
      "Train loss at 2000 is 0.19297434389591217)\n",
      "Train loss at 2000 is 0.19264386594295502)\n",
      "Train loss at 2000 is 0.1923142522573471)\n",
      "Train loss at 2000 is 0.19198551774024963)\n",
      "Train loss at 2000 is 0.19165761768817902)\n",
      "Train loss at 2000 is 0.19133234024047852)\n",
      "Train loss at 2000 is 0.19100728631019592)\n",
      "Train loss at 2000 is 0.1906847357749939)\n",
      "Train loss at 2000 is 0.1903616189956665)\n",
      "Train loss at 2000 is 0.190039724111557)\n",
      "Train loss at 2000 is 0.18971948325634003)\n",
      "Train loss at 2000 is 0.18939907848834991)\n",
      "Train loss at 2000 is 0.18908020853996277)\n",
      "Train loss at 2000 is 0.18876239657402039)\n",
      "Train loss at 2000 is 0.18844464421272278)\n",
      "Train loss at 2000 is 0.1881285309791565)\n",
      "Train loss at 2000 is 0.18781322240829468)\n",
      "Train loss at 2000 is 0.18749810755252838)\n",
      "Train loss at 2000 is 0.18718455731868744)\n",
      "Train loss at 2000 is 0.18687203526496887)\n",
      "Train loss at 2000 is 0.18655957281589508)\n",
      "Train loss at 2000 is 0.18624812364578247)\n",
      "Train loss at 2000 is 0.18593844771385193)\n",
      "Train loss at 2000 is 0.18562866747379303)\n",
      "Train loss at 2000 is 0.18531963229179382)\n",
      "Train loss at 2000 is 0.1850123107433319)\n",
      "Train loss at 2000 is 0.18470540642738342)\n",
      "Train loss at 2000 is 0.18439896404743195)\n",
      "Train loss at 2000 is 0.18409335613250732)\n",
      "Train loss at 2000 is 0.18378965556621552)\n",
      "Train loss at 2000 is 0.18348568677902222)\n",
      "Train loss at 2000 is 0.18318255245685577)\n",
      "Train loss at 2000 is 0.18288037180900574)\n",
      "Train loss at 2000 is 0.18257981538772583)\n",
      "Train loss at 2000 is 0.1822790950536728)\n",
      "Train loss at 2000 is 0.18198038637638092)\n",
      "Train loss at 2000 is 0.1816830039024353)\n",
      "Train loss at 2000 is 0.1813872754573822)\n",
      "Train loss at 2000 is 0.18109183013439178)\n",
      "Train loss at 2000 is 0.1807970106601715)\n",
      "Train loss at 2000 is 0.18050310015678406)\n",
      "Train loss at 2000 is 0.18021027743816376)\n",
      "Train loss at 2000 is 0.17991876602172852)\n",
      "Train loss at 2000 is 0.17962735891342163)\n",
      "Train loss at 2000 is 0.1793367862701416)\n",
      "Train loss at 2000 is 0.17904704809188843)\n",
      "Train loss at 2000 is 0.1787581443786621)\n",
      "Train loss at 2000 is 0.1784707009792328)\n",
      "Train loss at 2000 is 0.1781836748123169)\n",
      "Train loss at 2000 is 0.17789730429649353)\n",
      "Train loss at 2000 is 0.177611842751503)\n",
      "Train loss at 2000 is 0.17732708156108856)\n",
      "Train loss at 2000 is 0.17704319953918457)\n",
      "Train loss at 2000 is 0.17676039040088654)\n",
      "Train loss at 2000 is 0.17647837102413177)\n",
      "Train loss at 2000 is 0.17619681358337402)\n",
      "Train loss at 2000 is 0.1759161502122879)\n",
      "Train loss at 2000 is 0.1756362020969391)\n",
      "Train loss at 2000 is 0.17535702884197235)\n",
      "Train loss at 2000 is 0.17507867515087128)\n",
      "Train loss at 2000 is 0.1748010516166687)\n",
      "Train loss at 2000 is 0.1745244562625885)\n",
      "Train loss at 2000 is 0.1742485910654068)\n",
      "Train loss at 2000 is 0.17397329211235046)\n",
      "Train loss at 2000 is 0.17369875311851501)\n",
      "Train loss at 2000 is 0.17342492938041687)\n",
      "Train loss at 2000 is 0.1731519252061844)\n",
      "Train loss at 2000 is 0.1728796809911728)\n",
      "Train loss at 2000 is 0.17260810732841492)\n",
      "Train loss at 2000 is 0.17233730852603912)\n",
      "Train loss at 2000 is 0.17206723988056183)\n",
      "Train loss at 2000 is 0.17179793119430542)\n",
      "Train loss at 2000 is 0.17152927815914154)\n",
      "Train loss at 2000 is 0.1712614893913269)\n",
      "Train loss at 2000 is 0.17099449038505554)\n",
      "Train loss at 2000 is 0.17072802782058716)\n",
      "Train loss at 2000 is 0.17046236991882324)\n",
      "Train loss at 2000 is 0.17019733786582947)\n",
      "Train loss at 2000 is 0.1699329912662506)\n",
      "Train loss at 2000 is 0.16966934502124786)\n",
      "Train loss at 2000 is 0.16940650343894958)\n",
      "Train loss at 2000 is 0.16914424300193787)\n",
      "Train loss at 2000 is 0.16888272762298584)\n",
      "Train loss at 2000 is 0.1686219871044159)\n",
      "Train loss at 2000 is 0.1683618426322937)\n",
      "Train loss at 2000 is 0.1681024730205536)\n",
      "Train loss at 2000 is 0.16784366965293884)\n",
      "Train loss at 2000 is 0.1675855815410614)\n",
      "Train loss at 2000 is 0.16732817888259888)\n",
      "Train loss at 2000 is 0.16707143187522888)\n",
      "Train loss at 2000 is 0.16681542992591858)\n",
      "Train loss at 2000 is 0.16656000912189484)\n",
      "Train loss at 2000 is 0.1663052886724472)\n",
      "Train loss at 2000 is 0.1660512387752533)\n",
      "Train loss at 2000 is 0.16579782962799072)\n",
      "Train loss at 2000 is 0.16554509103298187)\n",
      "Train loss at 2000 is 0.16529296338558197)\n",
      "Train loss at 2000 is 0.16504152119159698)\n",
      "Train loss at 2000 is 0.16479073464870453)\n",
      "Train loss at 2000 is 0.16454057395458221)\n",
      "Train loss at 2000 is 0.16429105401039124)\n",
      "Train loss at 2000 is 0.1640421450138092)\n",
      "Train loss at 2000 is 0.1637939214706421)\n",
      "Train loss at 2000 is 0.16354627907276154)\n",
      "Train loss at 2000 is 0.1632992923259735)\n",
      "Train loss at 2000 is 0.16305294632911682)\n",
      "Train loss at 2000 is 0.16280712187290192)\n",
      "Train loss at 2000 is 0.1625620275735855)\n",
      "Train loss at 2000 is 0.16231754422187805)\n",
      "Train loss at 2000 is 0.16207358241081238)\n",
      "Train loss at 2000 is 0.16183032095432281)\n",
      "Train loss at 2000 is 0.161587655544281)\n",
      "Train loss at 2000 is 0.16134555637836456)\n",
      "Train loss at 2000 is 0.16110408306121826)\n",
      "Train loss at 2000 is 0.1608632504940033)\n",
      "Train loss at 2000 is 0.1606229543685913)\n",
      "Train loss at 2000 is 0.16038331389427185)\n",
      "Train loss at 2000 is 0.16014425456523895)\n",
      "Train loss at 2000 is 0.15990571677684784)\n",
      "Train loss at 2000 is 0.15966777503490448)\n",
      "Train loss at 2000 is 0.15943044424057007)\n",
      "Train loss at 2000 is 0.159193754196167)\n",
      "Train loss at 2000 is 0.1589575707912445)\n",
      "Train loss at 2000 is 0.15872201323509216)\n",
      "Train loss at 2000 is 0.1584869921207428)\n",
      "Train loss at 2000 is 0.1582525670528412)\n",
      "Train loss at 2000 is 0.15801867842674255)\n",
      "Train loss at 2000 is 0.15778544545173645)\n",
      "Train loss at 2000 is 0.15755268931388855)\n",
      "Train loss at 2000 is 0.1573205292224884)\n",
      "Train loss at 2000 is 0.15708892047405243)\n",
      "Train loss at 2000 is 0.15685787796974182)\n",
      "Train loss at 2000 is 0.1566273272037506)\n",
      "Train loss at 2000 is 0.15639740228652954)\n",
      "Train loss at 2000 is 0.15616801381111145)\n",
      "Train loss at 2000 is 0.15593916177749634)\n",
      "Train loss at 2000 is 0.15571095049381256)\n",
      "Train loss at 2000 is 0.1554831564426422)\n",
      "Train loss at 2000 is 0.1552559733390808)\n",
      "Train loss at 2000 is 0.1550293266773224)\n",
      "Train loss at 2000 is 0.15480320155620575)\n",
      "Train loss at 2000 is 0.15457764267921448)\n",
      "Train loss at 2000 is 0.15435254573822021)\n",
      "Train loss at 2000 is 0.1541280448436737)\n",
      "Train loss at 2000 is 0.1539040505886078)\n",
      "Train loss at 2000 is 0.15368059277534485)\n",
      "Train loss at 2000 is 0.15345771610736847)\n",
      "Train loss at 2000 is 0.15323524177074432)\n",
      "Train loss at 2000 is 0.15301339328289032)\n",
      "Train loss at 2000 is 0.15279197692871094)\n",
      "Train loss at 2000 is 0.1525711864233017)\n",
      "Train loss at 2000 is 0.15235081315040588)\n",
      "Train loss at 2000 is 0.15213099122047424)\n",
      "Train loss at 2000 is 0.15191170573234558)\n",
      "Train loss at 2000 is 0.15169289708137512)\n",
      "Train loss at 2000 is 0.15147462487220764)\n",
      "Train loss at 2000 is 0.15125682950019836)\n",
      "Train loss at 2000 is 0.15103957056999207)\n",
      "Train loss at 2000 is 0.15082278847694397)\n",
      "Train loss at 2000 is 0.15060648322105408)\n",
      "Train loss at 2000 is 0.15039069950580597)\n",
      "Train loss at 2000 is 0.15017543733119965)\n",
      "Train loss at 2000 is 0.1499606966972351)\n",
      "Train loss at 2000 is 0.1497463881969452)\n",
      "Train loss at 2000 is 0.14953258633613586)\n",
      "Train loss at 2000 is 0.14931926131248474)\n",
      "Train loss at 2000 is 0.1491064578294754)\n",
      "Train loss at 2000 is 0.14889408648014069)\n",
      "Train loss at 2000 is 0.14868226647377014)\n",
      "Train loss at 2000 is 0.14847087860107422)\n",
      "Train loss at 2000 is 0.1482599675655365)\n",
      "Train loss at 2000 is 0.14804959297180176)\n",
      "Train loss at 2000 is 0.14783968031406403)\n",
      "Train loss at 2000 is 0.14763018488883972)\n",
      "Train loss at 2000 is 0.147421196103096)\n",
      "Train loss at 2000 is 0.14721272885799408)\n",
      "Train loss at 2000 is 0.14700475335121155)\n",
      "Train loss at 2000 is 0.14679813385009766)\n",
      "Train loss at 2000 is 0.14659197628498077)\n",
      "Train loss at 2000 is 0.1463862806558609)\n",
      "Train loss at 2000 is 0.14618106186389923)\n",
      "Train loss at 2000 is 0.1459762156009674)\n",
      "Train loss at 2000 is 0.14577190577983856)\n",
      "Train loss at 2000 is 0.14556801319122314)\n",
      "Train loss at 2000 is 0.1453646719455719)\n",
      "Train loss at 2000 is 0.14516164362430573)\n",
      "Train loss at 2000 is 0.14495913684368134)\n",
      "Train loss at 2000 is 0.14475709199905396)\n",
      "Train loss at 2000 is 0.14455541968345642)\n",
      "Train loss at 2000 is 0.14435426890850067)\n",
      "Train loss at 2000 is 0.1441536247730255)\n",
      "Train loss at 2000 is 0.14395330846309662)\n",
      "Train loss at 2000 is 0.14375345408916473)\n",
      "Train loss at 2000 is 0.14355409145355225)\n",
      "Train loss at 2000 is 0.1433551013469696)\n",
      "Train loss at 2000 is 0.14315661787986755)\n",
      "Train loss at 2000 is 0.14295852184295654)\n",
      "Train loss at 2000 is 0.14276090264320374)\n",
      "Train loss at 2000 is 0.14256367087364197)\n",
      "Train loss at 2000 is 0.1423669010400772)\n",
      "Train loss at 2000 is 0.14217054843902588)\n",
      "Train loss at 2000 is 0.14197470247745514)\n",
      "Train loss at 2000 is 0.14177913963794708)\n",
      "Train loss at 2000 is 0.14158406853675842)\n",
      "Train loss at 2000 is 0.14138944447040558)\n",
      "Train loss at 2000 is 0.14119519293308258)\n",
      "Train loss at 2000 is 0.14100147783756256)\n",
      "Train loss at 2000 is 0.1408080756664276)\n",
      "Train loss at 2000 is 0.14061513543128967)\n",
      "Train loss at 2000 is 0.14042258262634277)\n",
      "Train loss at 2000 is 0.1402304768562317)\n",
      "Train loss at 2000 is 0.14003874361515045)\n",
      "Train loss at 2000 is 0.13984748721122742)\n",
      "Train loss at 2000 is 0.13965663313865662)\n",
      "Train loss at 2000 is 0.1394660919904709)\n",
      "Train loss at 2000 is 0.13927608728408813)\n",
      "Train loss at 2000 is 0.13908645510673523)\n",
      "Train loss at 2000 is 0.13889716565608978)\n",
      "Train loss at 2000 is 0.13870838284492493)\n",
      "Train loss at 2000 is 0.1385200023651123)\n",
      "Train loss at 2000 is 0.13833194971084595)\n",
      "Train loss at 2000 is 0.13814429938793182)\n",
      "Train loss at 2000 is 0.13795706629753113)\n",
      "Train loss at 2000 is 0.13777032494544983)\n",
      "Train loss at 2000 is 0.13758385181427002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.1373978555202484)\n",
      "Train loss at 2000 is 0.13721227645874023)\n",
      "Train loss at 2000 is 0.13702704012393951)\n",
      "Train loss at 2000 is 0.13684222102165222)\n",
      "Train loss at 2000 is 0.13665777444839478)\n",
      "Train loss at 2000 is 0.13647374510765076)\n",
      "Train loss at 2000 is 0.13629010319709778)\n",
      "Train loss at 2000 is 0.13610678911209106)\n",
      "Train loss at 2000 is 0.13592392206192017)\n",
      "Train loss at 2000 is 0.1357414424419403)\n",
      "Train loss at 2000 is 0.1355592906475067)\n",
      "Train loss at 2000 is 0.13537763059139252)\n",
      "Train loss at 2000 is 0.1351962685585022)\n",
      "Train loss at 2000 is 0.1350153535604477)\n",
      "Train loss at 2000 is 0.13483476638793945)\n",
      "Train loss at 2000 is 0.13465455174446106)\n",
      "Train loss at 2000 is 0.1344747245311737)\n",
      "Train loss at 2000 is 0.13429535925388336)\n",
      "Train loss at 2000 is 0.1341162621974945)\n",
      "Train loss at 2000 is 0.13393758237361908)\n",
      "Train loss at 2000 is 0.1337592899799347)\n",
      "Train loss at 2000 is 0.13358138501644135)\n",
      "Train loss at 2000 is 0.13340379297733307)\n",
      "Train loss at 2000 is 0.13322655856609344)\n",
      "Train loss at 2000 is 0.13304978609085083)\n",
      "Train loss at 2000 is 0.1328733265399933)\n",
      "Train loss at 2000 is 0.13269728422164917)\n",
      "Train loss at 2000 is 0.13252151012420654)\n",
      "Train loss at 2000 is 0.13234613835811615)\n",
      "Train loss at 2000 is 0.13217122852802277)\n",
      "Train loss at 2000 is 0.1319965422153473)\n",
      "Train loss at 2000 is 0.13182231783866882)\n",
      "Train loss at 2000 is 0.13164839148521423)\n",
      "Train loss at 2000 is 0.13147488236427307)\n",
      "Train loss at 2000 is 0.13130174577236176)\n",
      "Train loss at 2000 is 0.1311289519071579)\n",
      "Train loss at 2000 is 0.13095644116401672)\n",
      "Train loss at 2000 is 0.13078436255455017)\n",
      "Train loss at 2000 is 0.1306125819683075)\n",
      "Train loss at 2000 is 0.13044117391109467)\n",
      "Train loss at 2000 is 0.13027015328407288)\n",
      "Train loss at 2000 is 0.13009949028491974)\n",
      "Train loss at 2000 is 0.12992917001247406)\n",
      "Train loss at 2000 is 0.12975914776325226)\n",
      "Train loss at 2000 is 0.12958946824073792)\n",
      "Train loss at 2000 is 0.129420205950737)\n",
      "Train loss at 2000 is 0.12925127148628235)\n",
      "Train loss at 2000 is 0.12908269464969635)\n",
      "Train loss at 2000 is 0.12891443073749542)\n",
      "Train loss at 2000 is 0.12874652445316315)\n",
      "Train loss at 2000 is 0.12857894599437714)\n",
      "Train loss at 2000 is 0.12841179966926575)\n",
      "Train loss at 2000 is 0.12824490666389465)\n",
      "Train loss at 2000 is 0.12807834148406982)\n",
      "Train loss at 2000 is 0.12791213393211365)\n",
      "Train loss at 2000 is 0.1277463138103485)\n",
      "Train loss at 2000 is 0.12758077681064606)\n",
      "Train loss at 2000 is 0.12741558253765106)\n",
      "Train loss at 2000 is 0.1272507607936859)\n",
      "Train loss at 2000 is 0.12708623707294464)\n",
      "Train loss at 2000 is 0.12692208588123322)\n",
      "Train loss at 2000 is 0.12675823271274567)\n",
      "Train loss at 2000 is 0.12659472227096558)\n",
      "Train loss at 2000 is 0.12643155455589294)\n",
      "Train loss at 2000 is 0.1262686550617218)\n",
      "Train loss at 2000 is 0.12610618770122528)\n",
      "Train loss at 2000 is 0.12594395875930786)\n",
      "Train loss at 2000 is 0.12578214704990387)\n",
      "Train loss at 2000 is 0.12562060356140137)\n",
      "Train loss at 2000 is 0.12545935809612274)\n",
      "Train loss at 2000 is 0.12529850006103516)\n",
      "Train loss at 2000 is 0.12513795495033264)\n",
      "Train loss at 2000 is 0.1249777227640152)\n",
      "Train loss at 2000 is 0.12481778860092163)\n",
      "Train loss at 2000 is 0.12465822696685791)\n",
      "Train loss at 2000 is 0.12449900060892105)\n",
      "Train loss at 2000 is 0.1243399828672409)\n",
      "Train loss at 2000 is 0.12418140470981598)\n",
      "Train loss at 2000 is 0.12402304261922836)\n",
      "Train loss at 2000 is 0.12386508285999298)\n",
      "Train loss at 2000 is 0.1237073689699173)\n",
      "Train loss at 2000 is 0.12355005741119385)\n",
      "Train loss at 2000 is 0.1233929842710495)\n",
      "Train loss at 2000 is 0.12323625385761261)\n",
      "Train loss at 2000 is 0.12307979166507721)\n",
      "Train loss at 2000 is 0.12292370945215225)\n",
      "Train loss at 2000 is 0.12276816368103027)\n",
      "Train loss at 2000 is 0.1226128563284874)\n",
      "Train loss at 2000 is 0.12245786190032959)\n",
      "Train loss at 2000 is 0.12230329215526581)\n",
      "Train loss at 2000 is 0.12214889377355576)\n",
      "Train loss at 2000 is 0.12199483066797256)\n",
      "Train loss at 2000 is 0.12184109538793564)\n",
      "Train loss at 2000 is 0.12168768793344498)\n",
      "Train loss at 2000 is 0.12153454124927521)\n",
      "Train loss at 2000 is 0.12138166278600693)\n",
      "Train loss at 2000 is 0.12122917175292969)\n",
      "Train loss at 2000 is 0.12107698619365692)\n",
      "Train loss at 2000 is 0.12092505395412445)\n",
      "Train loss at 2000 is 0.12077345699071884)\n",
      "Train loss at 2000 is 0.12062212079763412)\n",
      "Train loss at 2000 is 0.12047113478183746)\n",
      "Train loss at 2000 is 0.1203203946352005)\n",
      "Train loss at 2000 is 0.1201699748635292)\n",
      "Train loss at 2000 is 0.1200198158621788)\n",
      "Train loss at 2000 is 0.11986999213695526)\n",
      "Train loss at 2000 is 0.1197204440832138)\n",
      "Train loss at 2000 is 0.11957122385501862)\n",
      "Train loss at 2000 is 0.1194223016500473)\n",
      "Train loss at 2000 is 0.11927364766597748)\n",
      "Train loss at 2000 is 0.11912528425455093)\n",
      "Train loss at 2000 is 0.11897717416286469)\n",
      "Train loss at 2000 is 0.11882944405078888)\n",
      "Train loss at 2000 is 0.11868195235729218)\n",
      "Train loss at 2000 is 0.11853474378585815)\n",
      "Train loss at 2000 is 0.1183878630399704)\n",
      "Train loss at 2000 is 0.11824119091033936)\n",
      "Train loss at 2000 is 0.11809490621089935)\n",
      "Train loss at 2000 is 0.11794884502887726)\n",
      "Train loss at 2000 is 0.11780311167240143)\n",
      "Train loss at 2000 is 0.1176576167345047)\n",
      "Train loss at 2000 is 0.11751244962215424)\n",
      "Train loss at 2000 is 0.11736752092838287)\n",
      "Train loss at 2000 is 0.11722288280725479)\n",
      "Train loss at 2000 is 0.11707856506109238)\n",
      "Train loss at 2000 is 0.11693446338176727)\n",
      "Train loss at 2000 is 0.11679066717624664)\n",
      "Train loss at 2000 is 0.11664720624685287)\n",
      "Train loss at 2000 is 0.1165039986371994)\n",
      "Train loss at 2000 is 0.11636105924844742)\n",
      "Train loss at 2000 is 0.11621837317943573)\n",
      "Train loss at 2000 is 0.1160760149359703)\n",
      "Train loss at 2000 is 0.11593389511108398)\n",
      "Train loss at 2000 is 0.11579209566116333)\n",
      "Train loss at 2000 is 0.1156507283449173)\n",
      "Train loss at 2000 is 0.11550964415073395)\n",
      "Train loss at 2000 is 0.1153688058257103)\n",
      "Train loss at 2000 is 0.11522828042507172)\n",
      "Train loss at 2000 is 0.11508794873952866)\n",
      "Train loss at 2000 is 0.11494793742895126)\n",
      "Train loss at 2000 is 0.11480820178985596)\n",
      "Train loss at 2000 is 0.11466872692108154)\n",
      "Train loss at 2000 is 0.1145295649766922)\n",
      "Train loss at 2000 is 0.11439061164855957)\n",
      "Train loss at 2000 is 0.11425194889307022)\n",
      "Train loss at 2000 is 0.11411358416080475)\n",
      "Train loss at 2000 is 0.11397542804479599)\n",
      "Train loss at 2000 is 0.1138375774025917)\n",
      "Train loss at 2000 is 0.11369995027780533)\n",
      "Train loss at 2000 is 0.11356264352798462)\n",
      "Train loss at 2000 is 0.11342553794384003)\n",
      "Train loss at 2000 is 0.11328872293233871)\n",
      "Train loss at 2000 is 0.11315222829580307)\n",
      "Train loss at 2000 is 0.11301594972610474)\n",
      "Train loss at 2000 is 0.1128799319267273)\n",
      "Train loss at 2000 is 0.11274409294128418)\n",
      "Train loss at 2000 is 0.11260859668254852)\n",
      "Train loss at 2000 is 0.11247336864471436)\n",
      "Train loss at 2000 is 0.11233842372894287)\n",
      "Train loss at 2000 is 0.1122036799788475)\n",
      "Train loss at 2000 is 0.11206921190023422)\n",
      "Train loss at 2000 is 0.11193499714136124)\n",
      "Train loss at 2000 is 0.11180102825164795)\n",
      "Train loss at 2000 is 0.11166731268167496)\n",
      "Train loss at 2000 is 0.11153386533260345)\n",
      "Train loss at 2000 is 0.11140067875385284)\n",
      "Train loss at 2000 is 0.11126774549484253)\n",
      "Train loss at 2000 is 0.11113506555557251)\n",
      "Train loss at 2000 is 0.11100264638662338)\n",
      "Train loss at 2000 is 0.11087048053741455)\n",
      "Train loss at 2000 is 0.11073864996433258)\n",
      "Train loss at 2000 is 0.11060699075460434)\n",
      "Train loss at 2000 is 0.110475555062294)\n",
      "Train loss at 2000 is 0.11034449189901352)\n",
      "Train loss at 2000 is 0.11021360009908676)\n",
      "Train loss at 2000 is 0.11008302122354507)\n",
      "Train loss at 2000 is 0.1099526509642601)\n",
      "Train loss at 2000 is 0.10982251167297363)\n",
      "Train loss at 2000 is 0.10969267040491104)\n",
      "Train loss at 2000 is 0.10956303030252457)\n",
      "Train loss at 2000 is 0.1094336286187172)\n",
      "Train loss at 2000 is 0.1093045100569725)\n",
      "Train loss at 2000 is 0.10917560756206512)\n",
      "Train loss at 2000 is 0.10904697328805923)\n",
      "Train loss at 2000 is 0.10891856253147125)\n",
      "Train loss at 2000 is 0.10879036039113998)\n",
      "Train loss at 2000 is 0.1086624413728714)\n",
      "Train loss at 2000 is 0.10853477567434311)\n",
      "Train loss at 2000 is 0.10840730369091034)\n",
      "Train loss at 2000 is 0.10828012228012085)\n",
      "Train loss at 2000 is 0.10815314948558807)\n",
      "Train loss at 2000 is 0.10802644491195679)\n",
      "Train loss at 2000 is 0.10789992660284042)\n",
      "Train loss at 2000 is 0.10777369886636734)\n",
      "Train loss at 2000 is 0.10764765739440918)\n",
      "Train loss at 2000 is 0.10752186924219131)\n",
      "Train loss at 2000 is 0.10739634186029434)\n",
      "Train loss at 2000 is 0.10727103054523468)\n",
      "Train loss at 2000 is 0.10714595019817352)\n",
      "Train loss at 2000 is 0.10702110826969147)\n",
      "Train loss at 2000 is 0.10689648240804672)\n",
      "Train loss at 2000 is 0.10677213966846466)\n",
      "Train loss at 2000 is 0.10664799064397812)\n",
      "Train loss at 2000 is 0.10652407258749008)\n",
      "Train loss at 2000 is 0.10640039294958115)\n",
      "Train loss at 2000 is 0.10627691447734833)\n",
      "Train loss at 2000 is 0.10615374147891998)\n",
      "Train loss at 2000 is 0.10603086650371552)\n",
      "Train loss at 2000 is 0.10590837150812149)\n",
      "Train loss at 2000 is 0.10578612238168716)\n",
      "Train loss at 2000 is 0.10566405206918716)\n",
      "Train loss at 2000 is 0.10554225742816925)\n",
      "Train loss at 2000 is 0.10542067140340805)\n",
      "Train loss at 2000 is 0.10529933124780655)\n",
      "Train loss at 2000 is 0.10517819225788116)\n",
      "Train loss at 2000 is 0.10505731403827667)\n",
      "Train loss at 2000 is 0.1049366146326065)\n",
      "Train loss at 2000 is 0.10481616109609604)\n",
      "Train loss at 2000 is 0.10469596087932587)\n",
      "Train loss at 2000 is 0.10457591712474823)\n",
      "Train loss at 2000 is 0.10445612668991089)\n",
      "Train loss at 2000 is 0.10433658212423325)\n",
      "Train loss at 2000 is 0.10421723127365112)\n",
      "Train loss at 2000 is 0.10409806668758392)\n",
      "Train loss at 2000 is 0.10397917032241821)\n",
      "Train loss at 2000 is 0.10386046022176743)\n",
      "Train loss at 2000 is 0.10374200344085693)\n",
      "Train loss at 2000 is 0.10362374782562256)\n",
      "Train loss at 2000 is 0.1035056859254837)\n",
      "Train loss at 2000 is 0.10338787734508514)\n",
      "Train loss at 2000 is 0.1032702699303627)\n",
      "Train loss at 2000 is 0.10315285623073578)\n",
      "Train loss at 2000 is 0.10303573310375214)\n",
      "Train loss at 2000 is 0.10291872918605804)\n",
      "Train loss at 2000 is 0.10280200093984604)\n",
      "Train loss at 2000 is 0.10268549621105194)\n",
      "Train loss at 2000 is 0.10256917774677277)\n",
      "Train loss at 2000 is 0.10245305299758911)\n",
      "Train loss at 2000 is 0.10233715921640396)\n",
      "Train loss at 2000 is 0.10222148895263672)\n",
      "Train loss at 2000 is 0.1021060198545456)\n",
      "Train loss at 2000 is 0.10199078172445297)\n",
      "Train loss at 2000 is 0.10187574476003647)\n",
      "Train loss at 2000 is 0.1017608642578125)\n",
      "Train loss at 2000 is 0.10164624452590942)\n",
      "Train loss at 2000 is 0.10153181850910187)\n",
      "Train loss at 2000 is 0.10141757875680923)\n",
      "Train loss at 2000 is 0.1013035774230957)\n",
      "Train loss at 2000 is 0.10118977725505829)\n",
      "Train loss at 2000 is 0.1010761633515358)\n",
      "Train loss at 2000 is 0.100962795317173)\n",
      "Train loss at 2000 is 0.10084960609674454)\n",
      "Train loss at 2000 is 0.10073666274547577)\n",
      "Train loss at 2000 is 0.10062389075756073)\n",
      "Train loss at 2000 is 0.100511334836483)\n",
      "Train loss at 2000 is 0.1003989577293396)\n",
      "Train loss at 2000 is 0.1002868041396141)\n",
      "Train loss at 2000 is 0.10017482936382294)\n",
      "Train loss at 2000 is 0.10006310790777206)\n",
      "Train loss at 2000 is 0.09995155781507492)\n",
      "Train loss at 2000 is 0.0998402014374733)\n",
      "Train loss at 2000 is 0.09972906857728958)\n",
      "Train loss at 2000 is 0.09961812198162079)\n",
      "Train loss at 2000 is 0.09950735419988632)\n",
      "Train loss at 2000 is 0.09939683228731155)\n",
      "Train loss at 2000 is 0.09928646683692932)\n",
      "Train loss at 2000 is 0.0991763323545456)\n",
      "Train loss at 2000 is 0.099066361784935)\n",
      "Train loss at 2000 is 0.09895661473274231)\n",
      "Train loss at 2000 is 0.09884707629680634)\n",
      "Train loss at 2000 is 0.0987376719713211)\n",
      "Train loss at 2000 is 0.09862855076789856)\n",
      "Train loss at 2000 is 0.09851960092782974)\n",
      "Train loss at 2000 is 0.09841082990169525)\n",
      "Train loss at 2000 is 0.09830230474472046)\n",
      "Train loss at 2000 is 0.09819391369819641)\n",
      "Train loss at 2000 is 0.09808571636676788)\n",
      "Train loss at 2000 is 0.09797774255275726)\n",
      "Train loss at 2000 is 0.09786996245384216)\n",
      "Train loss at 2000 is 0.0977623462677002)\n",
      "Train loss at 2000 is 0.09765496104955673)\n",
      "Train loss at 2000 is 0.0975477546453476)\n",
      "Train loss at 2000 is 0.097440704703331)\n",
      "Train loss at 2000 is 0.09733390063047409)\n",
      "Train loss at 2000 is 0.09722723066806793)\n",
      "Train loss at 2000 is 0.09712077677249908)\n",
      "Train loss at 2000 is 0.09701453894376755)\n",
      "Train loss at 2000 is 0.09690845012664795)\n",
      "Train loss at 2000 is 0.09680259972810745)\n",
      "Train loss at 2000 is 0.09669692814350128)\n",
      "Train loss at 2000 is 0.09659139811992645)\n",
      "Train loss at 2000 is 0.09648609161376953)\n",
      "Train loss at 2000 is 0.09638096392154694)\n",
      "Train loss at 2000 is 0.09627600014209747)\n",
      "Train loss at 2000 is 0.09617126733064651)\n",
      "Train loss at 2000 is 0.09606669843196869)\n",
      "Train loss at 2000 is 0.09596232324838638)\n",
      "Train loss at 2000 is 0.09585811197757721)\n",
      "Train loss at 2000 is 0.09575410932302475)\n",
      "Train loss at 2000 is 0.09565027058124542)\n",
      "Train loss at 2000 is 0.0955466479063034)\n",
      "Train loss at 2000 is 0.09544317424297333)\n",
      "Train loss at 2000 is 0.09533988684415817)\n",
      "Train loss at 2000 is 0.09523682296276093)\n",
      "Train loss at 2000 is 0.09513388574123383)\n",
      "Train loss at 2000 is 0.09503116458654404)\n",
      "Train loss at 2000 is 0.09492857754230499)\n",
      "Train loss at 2000 is 0.09482622891664505)\n",
      "Train loss at 2000 is 0.09472402930259705)\n",
      "Train loss at 2000 is 0.09462203830480576)\n",
      "Train loss at 2000 is 0.09452022612094879)\n",
      "Train loss at 2000 is 0.09441856294870377)\n",
      "Train loss at 2000 is 0.09431710094213486)\n",
      "Train loss at 2000 is 0.09421579539775848)\n",
      "Train loss at 2000 is 0.09411470592021942)\n",
      "Train loss at 2000 is 0.0940137505531311)\n",
      "Train loss at 2000 is 0.0939130038022995)\n",
      "Train loss at 2000 is 0.09381246566772461)\n",
      "Train loss at 2000 is 0.09371201694011688)\n",
      "Train loss at 2000 is 0.09361179172992706)\n",
      "Train loss at 2000 is 0.09351174533367157)\n",
      "Train loss at 2000 is 0.0934118777513504)\n",
      "Train loss at 2000 is 0.09331215918064117)\n",
      "Train loss at 2000 is 0.09321261942386627)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.09311329573392868)\n",
      "Train loss at 2000 is 0.09301413595676422)\n",
      "Train loss at 2000 is 0.0929151400923729)\n",
      "Train loss at 2000 is 0.0928163081407547)\n",
      "Train loss at 2000 is 0.09271761775016785)\n",
      "Train loss at 2000 is 0.09261913597583771)\n",
      "Train loss at 2000 is 0.0925208330154419)\n",
      "Train loss at 2000 is 0.09242269396781921)\n",
      "Train loss at 2000 is 0.09232470393180847)\n",
      "Train loss at 2000 is 0.09222690016031265)\n",
      "Train loss at 2000 is 0.09212929010391235)\n",
      "Train loss at 2000 is 0.0920318216085434)\n",
      "Train loss at 2000 is 0.09193453192710876)\n",
      "Train loss at 2000 is 0.09183740615844727)\n",
      "Train loss at 2000 is 0.0917404443025589)\n",
      "Train loss at 2000 is 0.09164369106292725)\n",
      "Train loss at 2000 is 0.09154705703258514)\n",
      "Train loss at 2000 is 0.09145060181617737)\n",
      "Train loss at 2000 is 0.09135432541370392)\n",
      "Train loss at 2000 is 0.0912582129240036)\n",
      "Train loss at 2000 is 0.09116226434707642)\n",
      "Train loss at 2000 is 0.09106649458408356)\n",
      "Train loss at 2000 is 0.09097086638212204)\n",
      "Train loss at 2000 is 0.09087542444467545)\n",
      "Train loss at 2000 is 0.09078015387058258)\n",
      "Train loss at 2000 is 0.09068499505519867)\n",
      "Train loss at 2000 is 0.09059005975723267)\n",
      "Train loss at 2000 is 0.090495266020298)\n",
      "Train loss at 2000 is 0.09040065109729767)\n",
      "Train loss at 2000 is 0.09030617773532867)\n",
      "Train loss at 2000 is 0.09021187573671341)\n",
      "Train loss at 2000 is 0.09011775255203247)\n",
      "Train loss at 2000 is 0.09002377837896347)\n",
      "Train loss at 2000 is 0.08992993831634521)\n",
      "Train loss at 2000 is 0.08983628451824188)\n",
      "Train loss at 2000 is 0.08974281698465347)\n",
      "Train loss at 2000 is 0.08964944630861282)\n",
      "Train loss at 2000 is 0.08955629169940948)\n",
      "Train loss at 2000 is 0.08946328610181808)\n",
      "Train loss at 2000 is 0.08937045186758041)\n",
      "Train loss at 2000 is 0.0892777293920517)\n",
      "Train loss at 2000 is 0.08918516337871552)\n",
      "Train loss at 2000 is 0.08909283578395844)\n",
      "Train loss at 2000 is 0.0890006422996521)\n",
      "Train loss at 2000 is 0.08890856802463531)\n",
      "Train loss at 2000 is 0.08881668746471405)\n",
      "Train loss at 2000 is 0.08872492611408234)\n",
      "Train loss at 2000 is 0.08863335102796555)\n",
      "Train loss at 2000 is 0.0885419100522995)\n",
      "Train loss at 2000 is 0.08845063298940659)\n",
      "Train loss at 2000 is 0.0883595421910286)\n",
      "Train loss at 2000 is 0.08826856315135956)\n",
      "Train loss at 2000 is 0.08817777782678604)\n",
      "Train loss at 2000 is 0.08808708935976028)\n",
      "Train loss at 2000 is 0.08799661695957184)\n",
      "Train loss at 2000 is 0.08790626376867294)\n",
      "Train loss at 2000 is 0.08781604468822479)\n",
      "Train loss at 2000 is 0.08772606402635574)\n",
      "Train loss at 2000 is 0.08763615787029266)\n",
      "Train loss at 2000 is 0.08754642307758331)\n",
      "Train loss at 2000 is 0.0874568447470665)\n",
      "Train loss at 2000 is 0.08736742287874222)\n",
      "Train loss at 2000 is 0.08727817982435226)\n",
      "Train loss at 2000 is 0.08718908578157425)\n",
      "Train loss at 2000 is 0.08710013329982758)\n",
      "Train loss at 2000 is 0.08701135963201523)\n",
      "Train loss at 2000 is 0.08692272752523422)\n",
      "Train loss at 2000 is 0.08683420717716217)\n",
      "Train loss at 2000 is 0.08674589544534683)\n",
      "Train loss at 2000 is 0.08665768802165985)\n",
      "Train loss at 2000 is 0.08656962960958481)\n",
      "Train loss at 2000 is 0.0864817425608635)\n",
      "Train loss at 2000 is 0.08639396727085114)\n",
      "Train loss at 2000 is 0.08630639314651489)\n",
      "Train loss at 2000 is 0.08621896803379059)\n",
      "Train loss at 2000 is 0.08613164722919464)\n",
      "Train loss at 2000 is 0.08604446798563004)\n",
      "Train loss at 2000 is 0.08595744520425797)\n",
      "Train loss at 2000 is 0.08587062358856201)\n",
      "Train loss at 2000 is 0.08578388392925262)\n",
      "Train loss at 2000 is 0.08569730818271637)\n",
      "Train loss at 2000 is 0.08561088889837265)\n",
      "Train loss at 2000 is 0.08552460372447968)\n",
      "Train loss at 2000 is 0.08543845266103745)\n",
      "Train loss at 2000 is 0.08535248786211014)\n",
      "Train loss at 2000 is 0.08526664227247238)\n",
      "Train loss at 2000 is 0.08518092334270477)\n",
      "Train loss at 2000 is 0.08509538322687149)\n",
      "Train loss at 2000 is 0.08500993251800537)\n",
      "Train loss at 2000 is 0.08492468297481537)\n",
      "Train loss at 2000 is 0.08483953773975372)\n",
      "Train loss at 2000 is 0.0847545713186264)\n",
      "Train loss at 2000 is 0.08466971665620804)\n",
      "Train loss at 2000 is 0.0845850259065628)\n",
      "Train loss at 2000 is 0.08450044691562653)\n",
      "Train loss at 2000 is 0.08441603928804398)\n",
      "Train loss at 2000 is 0.08433172851800919)\n",
      "Train loss at 2000 is 0.08424757421016693)\n",
      "Train loss at 2000 is 0.0841636061668396)\n",
      "Train loss at 2000 is 0.08407971262931824)\n",
      "Train loss at 2000 is 0.0839960128068924)\n",
      "Train loss at 2000 is 0.08391241729259491)\n",
      "Train loss at 2000 is 0.08382898569107056)\n",
      "Train loss at 2000 is 0.08374567329883575)\n",
      "Train loss at 2000 is 0.0836624950170517)\n",
      "Train loss at 2000 is 0.08357947319746017)\n",
      "Train loss at 2000 is 0.0834965929389)\n",
      "Train loss at 2000 is 0.08341386169195175)\n",
      "Train loss at 2000 is 0.08333124220371246)\n",
      "Train loss at 2000 is 0.08324877172708511)\n",
      "Train loss at 2000 is 0.08316643536090851)\n",
      "Train loss at 2000 is 0.08308419585227966)\n",
      "Train loss at 2000 is 0.08300211280584335)\n",
      "Train loss at 2000 is 0.08292020857334137)\n",
      "Train loss at 2000 is 0.08283837884664536)\n",
      "Train loss at 2000 is 0.08275672793388367)\n",
      "Train loss at 2000 is 0.08267519623041153)\n",
      "Train loss at 2000 is 0.08259378373622894)\n",
      "Train loss at 2000 is 0.08251255005598068)\n",
      "Train loss at 2000 is 0.08243142813444138)\n",
      "Train loss at 2000 is 0.08235041797161102)\n",
      "Train loss at 2000 is 0.0822695642709732)\n",
      "Train loss at 2000 is 0.08218883723020554)\n",
      "Train loss at 2000 is 0.08210824429988861)\n",
      "Train loss at 2000 is 0.08202777802944183)\n",
      "Train loss at 2000 is 0.0819474384188652)\n",
      "Train loss at 2000 is 0.0818672776222229)\n",
      "Train loss at 2000 is 0.08178719133138657)\n",
      "Train loss at 2000 is 0.08170724660158157)\n",
      "Train loss at 2000 is 0.08162745088338852)\n",
      "Train loss at 2000 is 0.08154778182506561)\n",
      "Train loss at 2000 is 0.08146826177835464)\n",
      "Train loss at 2000 is 0.08138881623744965)\n",
      "Train loss at 2000 is 0.08130954205989838)\n",
      "Train loss at 2000 is 0.08123040199279785)\n",
      "Train loss at 2000 is 0.08115137368440628)\n",
      "Train loss at 2000 is 0.08107248693704605)\n",
      "Train loss at 2000 is 0.08099369704723358)\n",
      "Train loss at 2000 is 0.08091507107019424)\n",
      "Train loss at 2000 is 0.08083656430244446)\n",
      "Train loss at 2000 is 0.08075816929340363)\n",
      "Train loss at 2000 is 0.08067993074655533)\n",
      "Train loss at 2000 is 0.08060182631015778)\n",
      "Train loss at 2000 is 0.08052381873130798)\n",
      "Train loss at 2000 is 0.08044593036174774)\n",
      "Train loss at 2000 is 0.08036818355321884)\n",
      "Train loss at 2000 is 0.08029057830572128)\n",
      "Train loss at 2000 is 0.08021308481693268)\n",
      "Train loss at 2000 is 0.08013571053743362)\n",
      "Train loss at 2000 is 0.08005847036838531)\n",
      "Train loss at 2000 is 0.07998134195804596)\n",
      "Train loss at 2000 is 0.07990437000989914)\n",
      "Train loss at 2000 is 0.07982750982046127)\n",
      "Train loss at 2000 is 0.07975076138973236)\n",
      "Train loss at 2000 is 0.0796741396188736)\n",
      "Train loss at 2000 is 0.07959764450788498)\n",
      "Train loss at 2000 is 0.07952127605676651)\n",
      "Train loss at 2000 is 0.07944504916667938)\n",
      "Train loss at 2000 is 0.07936893403530121)\n",
      "Train loss at 2000 is 0.07929293066263199)\n",
      "Train loss at 2000 is 0.07921706140041351)\n",
      "Train loss at 2000 is 0.07914133369922638)\n",
      "Train loss at 2000 is 0.07906566560268402)\n",
      "Train loss at 2000 is 0.07899017632007599)\n",
      "Train loss at 2000 is 0.07891476154327393)\n",
      "Train loss at 2000 is 0.07883953303098679)\n",
      "Train loss at 2000 is 0.0787644162774086)\n",
      "Train loss at 2000 is 0.078689344227314)\n",
      "Train loss at 2000 is 0.0786144807934761)\n",
      "Train loss at 2000 is 0.07853967696428299)\n",
      "Train loss at 2000 is 0.07846499979496002)\n",
      "Train loss at 2000 is 0.07839048653841019)\n",
      "Train loss at 2000 is 0.07831607758998871)\n",
      "Train loss at 2000 is 0.0782417431473732)\n",
      "Train loss at 2000 is 0.07816758006811142)\n",
      "Train loss at 2000 is 0.0780935287475586)\n",
      "Train loss at 2000 is 0.07801957428455353)\n",
      "Train loss at 2000 is 0.07794573903083801)\n",
      "Train loss at 2000 is 0.07787202298641205)\n",
      "Train loss at 2000 is 0.07779844105243683)\n",
      "Train loss at 2000 is 0.07772497087717056)\n",
      "Train loss at 2000 is 0.07765159755945206)\n",
      "Train loss at 2000 is 0.07757838815450668)\n",
      "Train loss at 2000 is 0.07750528305768967)\n",
      "Train loss at 2000 is 0.07743225991725922)\n",
      "Train loss at 2000 is 0.07735937088727951)\n",
      "Train loss at 2000 is 0.07728660106658936)\n",
      "Train loss at 2000 is 0.07721395045518875)\n",
      "Train loss at 2000 is 0.0771414190530777)\n",
      "Train loss at 2000 is 0.07706896960735321)\n",
      "Train loss at 2000 is 0.07699667662382126)\n",
      "Train loss at 2000 is 0.07692447304725647)\n",
      "Train loss at 2000 is 0.0768524706363678)\n",
      "Train loss at 2000 is 0.07678066939115524)\n",
      "Train loss at 2000 is 0.07670894265174866)\n",
      "Train loss at 2000 is 0.0766373798251152)\n",
      "Train loss at 2000 is 0.07656591385602951)\n",
      "Train loss at 2000 is 0.07649457454681396)\n",
      "Train loss at 2000 is 0.07642335444688797)\n",
      "Train loss at 2000 is 0.07635222375392914)\n",
      "Train loss at 2000 is 0.07628121972084045)\n",
      "Train loss at 2000 is 0.07621027529239655)\n",
      "Train loss at 2000 is 0.07613949477672577)\n",
      "Train loss at 2000 is 0.07606882601976395)\n",
      "Train loss at 2000 is 0.07599827647209167)\n",
      "Train loss at 2000 is 0.07592782378196716)\n",
      "Train loss at 2000 is 0.0758574977517128)\n",
      "Train loss at 2000 is 0.0757872611284256)\n",
      "Train loss at 2000 is 0.07571710646152496)\n",
      "Train loss at 2000 is 0.07564711570739746)\n",
      "Train loss at 2000 is 0.07557721436023712)\n",
      "Train loss at 2000 is 0.0755077451467514)\n",
      "Train loss at 2000 is 0.07543863356113434)\n",
      "Train loss at 2000 is 0.07536973059177399)\n",
      "Train loss at 2000 is 0.0753006562590599)\n",
      "Train loss at 2000 is 0.07523205876350403)\n",
      "Train loss at 2000 is 0.0751633271574974)\n",
      "Train loss at 2000 is 0.07509458065032959)\n",
      "Train loss at 2000 is 0.07502643018960953)\n",
      "Train loss at 2000 is 0.07495789229869843)\n",
      "Train loss at 2000 is 0.07488963752985)\n",
      "Train loss at 2000 is 0.07482165098190308)\n",
      "Train loss at 2000 is 0.07475344836711884)\n",
      "Train loss at 2000 is 0.07468565553426743)\n",
      "Train loss at 2000 is 0.07461784034967422)\n",
      "Train loss at 2000 is 0.07454995810985565)\n",
      "Train loss at 2000 is 0.07448263466358185)\n",
      "Train loss at 2000 is 0.07441506534814835)\n",
      "Train loss at 2000 is 0.07434752583503723)\n",
      "Train loss at 2000 is 0.07428058236837387)\n",
      "Train loss at 2000 is 0.07421322166919708)\n",
      "Train loss at 2000 is 0.0741460919380188)\n",
      "Train loss at 2000 is 0.07407940924167633)\n",
      "Train loss at 2000 is 0.07401235401630402)\n",
      "Train loss at 2000 is 0.07394564151763916)\n",
      "Train loss at 2000 is 0.0738791674375534)\n",
      "Train loss at 2000 is 0.07381248474121094)\n",
      "Train loss at 2000 is 0.07374609261751175)\n",
      "Train loss at 2000 is 0.07367991656064987)\n",
      "Train loss at 2000 is 0.07361356168985367)\n",
      "Train loss at 2000 is 0.07354748994112015)\n",
      "Train loss at 2000 is 0.07348161935806274)\n",
      "Train loss at 2000 is 0.0734155923128128)\n",
      "Train loss at 2000 is 0.07334984838962555)\n",
      "Train loss at 2000 is 0.07328426837921143)\n",
      "Train loss at 2000 is 0.07321856915950775)\n",
      "Train loss at 2000 is 0.0731530636548996)\n",
      "Train loss at 2000 is 0.07308785617351532)\n",
      "Train loss at 2000 is 0.07302242517471313)\n",
      "Train loss at 2000 is 0.07295724749565125)\n",
      "Train loss at 2000 is 0.07289239019155502)\n",
      "Train loss at 2000 is 0.07282727211713791)\n",
      "Train loss at 2000 is 0.07276232540607452)\n",
      "Train loss at 2000 is 0.07269781082868576)\n",
      "Train loss at 2000 is 0.07263302803039551)\n",
      "Train loss at 2000 is 0.07256834954023361)\n",
      "Train loss at 2000 is 0.07250417768955231)\n",
      "Train loss at 2000 is 0.07243970036506653)\n",
      "Train loss at 2000 is 0.0723753422498703)\n",
      "Train loss at 2000 is 0.07231131941080093)\n",
      "Train loss at 2000 is 0.07224728912115097)\n",
      "Train loss at 2000 is 0.07218322157859802)\n",
      "Train loss at 2000 is 0.07211937010288239)\n",
      "Train loss at 2000 is 0.07205577939748764)\n",
      "Train loss at 2000 is 0.07199202477931976)\n",
      "Train loss at 2000 is 0.07192835211753845)\n",
      "Train loss at 2000 is 0.07186517119407654)\n",
      "Train loss at 2000 is 0.07180172950029373)\n",
      "Train loss at 2000 is 0.07173836976289749)\n",
      "Train loss at 2000 is 0.07167528569698334)\n",
      "Train loss at 2000 is 0.07161230593919754)\n",
      "Train loss at 2000 is 0.07154923677444458)\n",
      "Train loss at 2000 is 0.07148626446723938)\n",
      "Train loss at 2000 is 0.07142380625009537)\n",
      "Train loss at 2000 is 0.0713609904050827)\n",
      "Train loss at 2000 is 0.07129832357168198)\n",
      "Train loss at 2000 is 0.07123592495918274)\n",
      "Train loss at 2000 is 0.07117366045713425)\n",
      "Train loss at 2000 is 0.0711112692952156)\n",
      "Train loss at 2000 is 0.07104898989200592)\n",
      "Train loss at 2000 is 0.07098708301782608)\n",
      "Train loss at 2000 is 0.07092506438493729)\n",
      "Train loss at 2000 is 0.07086309045553207)\n",
      "Train loss at 2000 is 0.07080123573541641)\n",
      "Train loss at 2000 is 0.0707397609949112)\n",
      "Train loss at 2000 is 0.07067804783582687)\n",
      "Train loss at 2000 is 0.07061643898487091)\n",
      "Train loss at 2000 is 0.07055504620075226)\n",
      "Train loss at 2000 is 0.07049387693405151)\n",
      "Train loss at 2000 is 0.07043254375457764)\n",
      "Train loss at 2000 is 0.07037131488323212)\n",
      "Train loss at 2000 is 0.07031036168336868)\n",
      "Train loss at 2000 is 0.07025017589330673)\n",
      "Train loss at 2000 is 0.07018963992595673)\n",
      "Train loss at 2000 is 0.07012972980737686)\n",
      "Train loss at 2000 is 0.07006963342428207)\n",
      "Train loss at 2000 is 0.07000966370105743)\n",
      "Train loss at 2000 is 0.06994997709989548)\n",
      "Train loss at 2000 is 0.06988993287086487)\n",
      "Train loss at 2000 is 0.06983041763305664)\n",
      "Train loss at 2000 is 0.06977103650569916)\n",
      "Train loss at 2000 is 0.06971126794815063)\n",
      "Train loss at 2000 is 0.06965204328298569)\n",
      "Train loss at 2000 is 0.0695926696062088)\n",
      "Train loss at 2000 is 0.06953342258930206)\n",
      "Train loss at 2000 is 0.06947460025548935)\n",
      "Train loss at 2000 is 0.06941531598567963)\n",
      "Train loss at 2000 is 0.06935625523328781)\n",
      "Train loss at 2000 is 0.06929786503314972)\n",
      "Train loss at 2000 is 0.06923887133598328)\n",
      "Train loss at 2000 is 0.06918016076087952)\n",
      "Train loss at 2000 is 0.06912167370319366)\n",
      "Train loss at 2000 is 0.06906308233737946)\n",
      "Train loss at 2000 is 0.06900490820407867)\n",
      "Train loss at 2000 is 0.06894662976264954)\n",
      "Train loss at 2000 is 0.06888817250728607)\n",
      "Train loss at 2000 is 0.06883016228675842)\n",
      "Train loss at 2000 is 0.06877235323190689)\n",
      "Train loss at 2000 is 0.0687141939997673)\n",
      "Train loss at 2000 is 0.06865648180246353)\n",
      "Train loss at 2000 is 0.06859861314296722)\n",
      "Train loss at 2000 is 0.06854076683521271)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.0684836134314537)\n",
      "Train loss at 2000 is 0.0684259757399559)\n",
      "Train loss at 2000 is 0.0683683529496193)\n",
      "Train loss at 2000 is 0.06831122934818268)\n",
      "Train loss at 2000 is 0.06825395673513412)\n",
      "Train loss at 2000 is 0.06819675862789154)\n",
      "Train loss at 2000 is 0.0681399255990982)\n",
      "Train loss at 2000 is 0.06808282434940338)\n",
      "Train loss at 2000 is 0.06802573800086975)\n",
      "Train loss at 2000 is 0.06796915829181671)\n",
      "Train loss at 2000 is 0.06791254132986069)\n",
      "Train loss at 2000 is 0.06785570830106735)\n",
      "Train loss at 2000 is 0.06779932975769043)\n",
      "Train loss at 2000 is 0.06774282455444336)\n",
      "Train loss at 2000 is 0.06768627464771271)\n",
      "Train loss at 2000 is 0.06763030588626862)\n",
      "Train loss at 2000 is 0.06757410615682602)\n",
      "Train loss at 2000 is 0.06751780211925507)\n",
      "Train loss at 2000 is 0.06746182590723038)\n",
      "Train loss at 2000 is 0.06740596145391464)\n",
      "Train loss at 2000 is 0.06735001504421234)\n",
      "Train loss at 2000 is 0.06729428470134735)\n",
      "Train loss at 2000 is 0.06723876297473907)\n",
      "Train loss at 2000 is 0.0671830102801323)\n",
      "Train loss at 2000 is 0.0671273022890091)\n",
      "Train loss at 2000 is 0.06707218289375305)\n",
      "Train loss at 2000 is 0.06701677292585373)\n",
      "Train loss at 2000 is 0.0669613927602768)\n",
      "Train loss at 2000 is 0.0669064074754715)\n",
      "Train loss at 2000 is 0.06685122847557068)\n",
      "Train loss at 2000 is 0.06679604947566986)\n",
      "Train loss at 2000 is 0.06674113124608994)\n",
      "Train loss at 2000 is 0.0666864737868309)\n",
      "Train loss at 2000 is 0.06663163006305695)\n",
      "Train loss at 2000 is 0.06657682359218597)\n",
      "Train loss at 2000 is 0.06652246415615082)\n",
      "Train loss at 2000 is 0.0664677768945694)\n",
      "Train loss at 2000 is 0.06641317903995514)\n",
      "Train loss at 2000 is 0.06635899096727371)\n",
      "Train loss at 2000 is 0.06630485504865646)\n",
      "Train loss at 2000 is 0.06625048816204071)\n",
      "Train loss at 2000 is 0.06619632244110107)\n",
      "Train loss at 2000 is 0.06614246219396591)\n",
      "Train loss at 2000 is 0.06608834862709045)\n",
      "Train loss at 2000 is 0.06603431701660156)\n",
      "Train loss at 2000 is 0.06598072499036789)\n",
      "Train loss at 2000 is 0.06592712551355362)\n",
      "Train loss at 2000 is 0.06587333977222443)\n",
      "Train loss at 2000 is 0.06581971049308777)\n",
      "Train loss at 2000 is 0.06576643139123917)\n",
      "Train loss at 2000 is 0.06571287661790848)\n",
      "Train loss at 2000 is 0.06565944850444794)\n",
      "Train loss at 2000 is 0.06560631841421127)\n",
      "Train loss at 2000 is 0.06555329263210297)\n",
      "Train loss at 2000 is 0.06550011038780212)\n",
      "Train loss at 2000 is 0.06544695049524307)\n",
      "Train loss at 2000 is 0.0653943121433258)\n",
      "Train loss at 2000 is 0.06534132361412048)\n",
      "Train loss at 2000 is 0.06528843939304352)\n",
      "Train loss at 2000 is 0.06523571908473969)\n",
      "Train loss at 2000 is 0.06518325954675674)\n",
      "Train loss at 2000 is 0.06513075530529022)\n",
      "Train loss at 2000 is 0.06507818400859833)\n",
      "Train loss at 2000 is 0.06502585113048553)\n",
      "Train loss at 2000 is 0.0649736225605011)\n",
      "Train loss at 2000 is 0.0649212896823883)\n",
      "Train loss at 2000 is 0.0648689940571785)\n",
      "Train loss at 2000 is 0.06481706351041794)\n",
      "Train loss at 2000 is 0.0647650882601738)\n",
      "Train loss at 2000 is 0.0647130161523819)\n",
      "Train loss at 2000 is 0.0646611750125885)\n",
      "Train loss at 2000 is 0.06460960954427719)\n",
      "Train loss at 2000 is 0.06455788761377335)\n",
      "Train loss at 2000 is 0.06450612843036652)\n",
      "Train loss at 2000 is 0.06445452570915222)\n",
      "Train loss at 2000 is 0.0644032210111618)\n",
      "Train loss at 2000 is 0.06435177475214005)\n",
      "Train loss at 2000 is 0.06430036574602127)\n",
      "Train loss at 2000 is 0.06424899399280548)\n",
      "Train loss at 2000 is 0.06419800221920013)\n",
      "Train loss at 2000 is 0.06414694339036942)\n",
      "Train loss at 2000 is 0.0640958622097969)\n",
      "Train loss at 2000 is 0.06404482573270798)\n",
      "Train loss at 2000 is 0.06399408727884293)\n",
      "Train loss at 2000 is 0.0639432892203331)\n",
      "Train loss at 2000 is 0.06389246135950089)\n",
      "Train loss at 2000 is 0.06384170800447464)\n",
      "Train loss at 2000 is 0.06379123032093048)\n",
      "Train loss at 2000 is 0.06374076753854752)\n",
      "Train loss at 2000 is 0.06369023025035858)\n",
      "Train loss at 2000 is 0.06363977491855621)\n",
      "Train loss at 2000 is 0.06358951330184937)\n",
      "Train loss at 2000 is 0.0635395273566246)\n",
      "Train loss at 2000 is 0.06348928064107895)\n",
      "Train loss at 2000 is 0.06343913078308105)\n",
      "Train loss at 2000 is 0.06338904798030853)\n",
      "Train loss at 2000 is 0.06333933770656586)\n",
      "Train loss at 2000 is 0.06328941136598587)\n",
      "Train loss at 2000 is 0.06323955208063126)\n",
      "Train loss at 2000 is 0.06318975985050201)\n",
      "Train loss at 2000 is 0.06314016133546829)\n",
      "Train loss at 2000 is 0.06309068948030472)\n",
      "Train loss at 2000 is 0.063041090965271)\n",
      "Train loss at 2000 is 0.06299157440662384)\n",
      "Train loss at 2000 is 0.06294216215610504)\n",
      "Train loss at 2000 is 0.0628931075334549)\n",
      "Train loss at 2000 is 0.06284381449222565)\n",
      "Train loss at 2000 is 0.06279464811086655)\n",
      "Train loss at 2000 is 0.06274548172950745)\n",
      "Train loss at 2000 is 0.06269653886556625)\n",
      "Train loss at 2000 is 0.06264768540859222)\n",
      "Train loss at 2000 is 0.06259874999523163)\n",
      "Train loss at 2000 is 0.0625499039888382)\n",
      "Train loss at 2000 is 0.06250107288360596)\n",
      "Train loss at 2000 is 0.062452565878629684)\n",
      "Train loss at 2000 is 0.06240401417016983)\n",
      "Train loss at 2000 is 0.062355391681194305)\n",
      "Train loss at 2000 is 0.062306880950927734)\n",
      "Train loss at 2000 is 0.06225844472646713)\n",
      "Train loss at 2000 is 0.06221028044819832)\n",
      "Train loss at 2000 is 0.062161993235349655)\n",
      "Train loss at 2000 is 0.062113769352436066)\n",
      "Train loss at 2000 is 0.06206559017300606)\n",
      "Train loss at 2000 is 0.06201748922467232)\n",
      "Train loss at 2000 is 0.06196969747543335)\n",
      "Train loss at 2000 is 0.06192176789045334)\n",
      "Train loss at 2000 is 0.061873890459537506)\n",
      "Train loss at 2000 is 0.061826031655073166)\n",
      "Train loss at 2000 is 0.061778269708156586)\n",
      "Train loss at 2000 is 0.06173077970743179)\n",
      "Train loss at 2000 is 0.06168322637677193)\n",
      "Train loss at 2000 is 0.06163564324378967)\n",
      "Train loss at 2000 is 0.06158817559480667)\n",
      "Train loss at 2000 is 0.06154076009988785)\n",
      "Train loss at 2000 is 0.061493538320064545)\n",
      "Train loss at 2000 is 0.061446379870176315)\n",
      "Train loss at 2000 is 0.06139911338686943)\n",
      "Train loss at 2000 is 0.06135197728872299)\n",
      "Train loss at 2000 is 0.06130487844347954)\n",
      "Train loss at 2000 is 0.06125789135694504)\n",
      "Train loss at 2000 is 0.061211150139570236)\n",
      "Train loss at 2000 is 0.061164241284132004)\n",
      "Train loss at 2000 is 0.06111740320920944)\n",
      "Train loss at 2000 is 0.061070628464221954)\n",
      "Train loss at 2000 is 0.061023958027362823)\n",
      "Train loss at 2000 is 0.06097748130559921)\n",
      "Train loss at 2000 is 0.060930974781513214)\n",
      "Train loss at 2000 is 0.06088448315858841)\n",
      "Train loss at 2000 is 0.060838036239147186)\n",
      "Train loss at 2000 is 0.06079166382551193)\n",
      "Train loss at 2000 is 0.06074537709355354)\n",
      "Train loss at 2000 is 0.060699351131916046)\n",
      "Train loss at 2000 is 0.06065315753221512)\n",
      "Train loss at 2000 is 0.06060704588890076)\n",
      "Train loss at 2000 is 0.06056102365255356)\n",
      "Train loss at 2000 is 0.06051502749323845)\n",
      "Train loss at 2000 is 0.0604691319167614)\n",
      "Train loss at 2000 is 0.06042347103357315)\n",
      "Train loss at 2000 is 0.06037767603993416)\n",
      "Train loss at 2000 is 0.06033195182681084)\n",
      "Train loss at 2000 is 0.06028628349304199)\n",
      "Train loss at 2000 is 0.06024067476391792)\n",
      "Train loss at 2000 is 0.06019516661763191)\n",
      "Train loss at 2000 is 0.060149867087602615)\n",
      "Train loss at 2000 is 0.060104478150606155)\n",
      "Train loss at 2000 is 0.06005910784006119)\n",
      "Train loss at 2000 is 0.060013823211193085)\n",
      "Train loss at 2000 is 0.05996859073638916)\n",
      "Train loss at 2000 is 0.0599234476685524)\n",
      "Train loss at 2000 is 0.05987849086523056)\n",
      "Train loss at 2000 is 0.05983351916074753)\n",
      "Train loss at 2000 is 0.05978851392865181)\n",
      "Train loss at 2000 is 0.05974360555410385)\n",
      "Train loss at 2000 is 0.05969874933362007)\n",
      "Train loss at 2000 is 0.05965394899249077)\n",
      "Train loss at 2000 is 0.05960928648710251)\n",
      "Train loss at 2000 is 0.05956476926803589)\n",
      "Train loss at 2000 is 0.05952014401555061)\n",
      "Train loss at 2000 is 0.0594756081700325)\n",
      "Train loss at 2000 is 0.05943113565444946)\n",
      "Train loss at 2000 is 0.05938670039176941)\n",
      "Train loss at 2000 is 0.05934234708547592)\n",
      "Train loss at 2000 is 0.05929812788963318)\n",
      "Train loss at 2000 is 0.0592540018260479)\n",
      "Train loss at 2000 is 0.059209804981946945)\n",
      "Train loss at 2000 is 0.05916566774249077)\n",
      "Train loss at 2000 is 0.059121619910001755)\n",
      "Train loss at 2000 is 0.059077613055706024)\n",
      "Train loss at 2000 is 0.05903369188308716)\n",
      "Train loss at 2000 is 0.05898985266685486)\n",
      "Train loss at 2000 is 0.05894613265991211)\n",
      "Train loss at 2000 is 0.05890238285064697)\n",
      "Train loss at 2000 is 0.058858685195446014)\n",
      "Train loss at 2000 is 0.05881505087018013)\n",
      "Train loss at 2000 is 0.05877145007252693)\n",
      "Train loss at 2000 is 0.058727920055389404)\n",
      "Train loss at 2000 is 0.058684445917606354)\n",
      "Train loss at 2000 is 0.05864117294549942)\n",
      "Train loss at 2000 is 0.058597881346940994)\n",
      "Train loss at 2000 is 0.058554597198963165)\n",
      "Train loss at 2000 is 0.05851132422685623)\n",
      "Train loss at 2000 is 0.05846817046403885)\n",
      "Train loss at 2000 is 0.05842500925064087)\n",
      "Train loss at 2000 is 0.05838196352124214)\n",
      "Train loss at 2000 is 0.058338992297649384)\n",
      "Train loss at 2000 is 0.05829617381095886)\n",
      "Train loss at 2000 is 0.05825331062078476)\n",
      "Train loss at 2000 is 0.05821046233177185)\n",
      "Train loss at 2000 is 0.058167703449726105)\n",
      "Train loss at 2000 is 0.05812498927116394)\n",
      "Train loss at 2000 is 0.05808231979608536)\n",
      "Train loss at 2000 is 0.058039743453264236)\n",
      "Train loss at 2000 is 0.0579971969127655)\n",
      "Train loss at 2000 is 0.057954780757427216)\n",
      "Train loss at 2000 is 0.0579124316573143)\n",
      "Train loss at 2000 is 0.0578700415790081)\n",
      "Train loss at 2000 is 0.057827722281217575)\n",
      "Train loss at 2000 is 0.05778546258807182)\n",
      "Train loss at 2000 is 0.05774326249957085)\n",
      "Train loss at 2000 is 0.057701122015714645)\n",
      "Train loss at 2000 is 0.057659029960632324)\n",
      "Train loss at 2000 is 0.05761700123548508)\n",
      "Train loss at 2000 is 0.05757511407136917)\n",
      "Train loss at 2000 is 0.05753322318196297)\n",
      "Train loss at 2000 is 0.05749136209487915)\n",
      "Train loss at 2000 is 0.05744954198598862)\n",
      "Train loss at 2000 is 0.05740779638290405)\n",
      "Train loss at 2000 is 0.05736609175801277)\n",
      "Train loss at 2000 is 0.057324446737766266)\n",
      "Train loss at 2000 is 0.05728284269571304)\n",
      "Train loss at 2000 is 0.05724131315946579)\n",
      "Train loss at 2000 is 0.05719982460141182)\n",
      "Train loss at 2000 is 0.057158540934324265)\n",
      "Train loss at 2000 is 0.05711715668439865)\n",
      "Train loss at 2000 is 0.0570758581161499)\n",
      "Train loss at 2000 is 0.057034581899642944)\n",
      "Train loss at 2000 is 0.05699337273836136)\n",
      "Train loss at 2000 is 0.05695221945643425)\n",
      "Train loss at 2000 is 0.05691109970211983)\n",
      "Train loss at 2000 is 0.056870050728321075)\n",
      "Train loss at 2000 is 0.0568290650844574)\n",
      "Train loss at 2000 is 0.056788116693496704)\n",
      "Train loss at 2000 is 0.05674729868769646)\n",
      "Train loss at 2000 is 0.0567064993083477)\n",
      "Train loss at 2000 is 0.056665729731321335)\n",
      "Train loss at 2000 is 0.05662503093481064)\n",
      "Train loss at 2000 is 0.05658431723713875)\n",
      "Train loss at 2000 is 0.056543707847595215)\n",
      "Train loss at 2000 is 0.05650312826037407)\n",
      "Train loss at 2000 is 0.05646263808012009)\n",
      "Train loss at 2000 is 0.056422166526317596)\n",
      "Train loss at 2000 is 0.056381773203611374)\n",
      "Train loss at 2000 is 0.056341398507356644)\n",
      "Train loss at 2000 is 0.056301116943359375)\n",
      "Train loss at 2000 is 0.056260932236909866)\n",
      "Train loss at 2000 is 0.05622076243162155)\n",
      "Train loss at 2000 is 0.056180618703365326)\n",
      "Train loss at 2000 is 0.05614052340388298)\n",
      "Train loss at 2000 is 0.05610046535730362)\n",
      "Train loss at 2000 is 0.056060485541820526)\n",
      "Train loss at 2000 is 0.056020546704530716)\n",
      "Train loss at 2000 is 0.055980660021305084)\n",
      "Train loss at 2000 is 0.05594082549214363)\n",
      "Train loss at 2000 is 0.05590103194117546)\n",
      "Train loss at 2000 is 0.05586131662130356)\n",
      "Train loss at 2000 is 0.05582161620259285)\n",
      "Train loss at 2000 is 0.05578199028968811)\n",
      "Train loss at 2000 is 0.05574247986078262)\n",
      "Train loss at 2000 is 0.05570297688245773)\n",
      "Train loss at 2000 is 0.05566348880529404)\n",
      "Train loss at 2000 is 0.05562404915690422)\n",
      "Train loss at 2000 is 0.055584680289030075)\n",
      "Train loss at 2000 is 0.05554535239934921)\n",
      "Train loss at 2000 is 0.055506087839603424)\n",
      "Train loss at 2000 is 0.05546686053276062)\n",
      "Train loss at 2000 is 0.0554276704788208)\n",
      "Train loss at 2000 is 0.055388547480106354)\n",
      "Train loss at 2000 is 0.055349480360746384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss at 2000 is 0.055310457944869995)\n",
      "Train loss at 2000 is 0.05527148395776749)\n",
      "Train loss at 2000 is 0.05523255467414856)\n",
      "Train loss at 2000 is 0.05519367381930351)\n",
      "Train loss at 2000 is 0.055154867470264435)\n",
      "Train loss at 2000 is 0.055116139352321625)\n",
      "Train loss at 2000 is 0.05507741495966911)\n",
      "Train loss at 2000 is 0.05503874272108078)\n",
      "Train loss at 2000 is 0.05500010773539543)\n",
      "Train loss at 2000 is 0.054961543530225754)\n",
      "Train loss at 2000 is 0.054923005402088165)\n",
      "Train loss at 2000 is 0.054884523153305054)\n",
      "Train loss at 2000 is 0.054846085608005524)\n",
      "Train loss at 2000 is 0.05480773001909256)\n",
      "Train loss at 2000 is 0.054769366979599)\n",
      "Train loss at 2000 is 0.05473107844591141)\n",
      "Train loss at 2000 is 0.05469285324215889)\n",
      "Train loss at 2000 is 0.054654646664857864)\n",
      "Train loss at 2000 is 0.054616499692201614)\n",
      "Train loss at 2000 is 0.054578423500061035)\n",
      "Train loss at 2000 is 0.054540377110242844)\n",
      "Train loss at 2000 is 0.05450240522623062)\n",
      "Train loss at 2000 is 0.05446445196866989)\n",
      "Train loss at 2000 is 0.054426539689302444)\n",
      "Train loss at 2000 is 0.05438872054219246)\n",
      "Train loss at 2000 is 0.054350949823856354)\n",
      "Train loss at 2000 is 0.05431317165493965)\n",
      "Train loss at 2000 is 0.05427547171711922)\n",
      "Train loss at 2000 is 0.05423782020807266)\n",
      "Train loss at 2000 is 0.054200179874897)\n",
      "Train loss at 2000 is 0.05416262149810791)\n",
      "Train loss at 2000 is 0.0541251115500927)\n",
      "Train loss at 2000 is 0.05408763885498047)\n",
      "Train loss at 2000 is 0.054050225764513016)\n",
      "Train loss at 2000 is 0.054012857377529144)\n",
      "Train loss at 2000 is 0.053975485265254974)\n",
      "Train loss at 2000 is 0.05393820255994797)\n",
      "Train loss at 2000 is 0.05390099436044693)\n",
      "Train loss at 2000 is 0.053863782435655594)\n",
      "Train loss at 2000 is 0.05382663756608963)\n",
      "Train loss at 2000 is 0.053789544850587845)\n",
      "Train loss at 2000 is 0.05375249311327934)\n",
      "Train loss at 2000 is 0.05371547490358353)\n",
      "Train loss at 2000 is 0.05367851257324219)\n",
      "Train loss at 2000 is 0.053641606122255325)\n",
      "Train loss at 2000 is 0.05360471457242966)\n",
      "Train loss at 2000 is 0.05356789380311966)\n",
      "Train loss at 2000 is 0.05353115126490593)\n",
      "Train loss at 2000 is 0.053494423627853394)\n",
      "Train loss at 2000 is 0.05345773696899414)\n",
      "Train loss at 2000 is 0.053421080112457275)\n",
      "Train loss at 2000 is 0.053384482860565186)\n",
      "Train loss at 2000 is 0.05334792286157608)\n",
      "Train loss at 2000 is 0.053311415016651154)\n",
      "Train loss at 2000 is 0.05327495187520981)\n",
      "Train loss at 2000 is 0.05323855206370354)\n",
      "Train loss at 2000 is 0.053202200680971146)\n",
      "Train loss at 2000 is 0.053165871649980545)\n",
      "Train loss at 2000 is 0.05312960594892502)\n",
      "Train loss at 2000 is 0.053093381226062775)\n",
      "Train loss at 2000 is 0.053057193756103516)\n",
      "Train loss at 2000 is 0.05302105098962784)\n",
      "Train loss at 2000 is 0.05298493430018425)\n",
      "Train loss at 2000 is 0.05294889211654663)\n",
      "Train loss at 2000 is 0.0529128722846508)\n",
      "Train loss at 2000 is 0.05287691205739975)\n",
      "Train loss at 2000 is 0.05284098908305168)\n",
      "Train loss at 2000 is 0.052805095911026)\n",
      "Train loss at 2000 is 0.052769262343645096)\n",
      "Train loss at 2000 is 0.052733469754457474)\n",
      "Train loss at 2000 is 0.052697718143463135)\n",
      "Train loss at 2000 is 0.052662014961242676)\n",
      "Train loss at 2000 is 0.0526263490319252)\n",
      "Train loss at 2000 is 0.052590735256671906)\n",
      "Train loss at 2000 is 0.05255516618490219)\n",
      "Train loss at 2000 is 0.052519649267196655)\n",
      "Train loss at 2000 is 0.052484143525362015)\n",
      "Train loss at 2000 is 0.05244871973991394)\n",
      "Train loss at 2000 is 0.052413295954465866)\n",
      "Train loss at 2000 is 0.052377961575984955)\n",
      "Train loss at 2000 is 0.052342623472213745)\n",
      "Train loss at 2000 is 0.05230734869837761)\n",
      "Train loss at 2000 is 0.05227212980389595)\n",
      "Train loss at 2000 is 0.05223695561289787)\n",
      "Train loss at 2000 is 0.05220180004835129)\n",
      "Train loss at 2000 is 0.05216670781373978)\n",
      "Train loss at 2000 is 0.052131664007902145)\n",
      "Train loss at 2000 is 0.052096642553806305)\n",
      "Train loss at 2000 is 0.05206165835261345)\n",
      "Train loss at 2000 is 0.052026718854904175)\n",
      "Train loss at 2000 is 0.05199183151125908)\n",
      "Train loss at 2000 is 0.051957011222839355)\n",
      "Train loss at 2000 is 0.05192219465970993)\n",
      "Train loss at 2000 is 0.05188743397593498)\n",
      "Train loss at 2000 is 0.05185271427035332)\n",
      "Train loss at 2000 is 0.051818035542964935)\n",
      "Train loss at 2000 is 0.05178340524435043)\n",
      "Train loss at 2000 is 0.05174883082509041)\n",
      "Train loss at 2000 is 0.05171426385641098)\n",
      "Train loss at 2000 is 0.051679760217666626)\n",
      "Train loss at 2000 is 0.051645297557115555)\n",
      "Train loss at 2000 is 0.05161086469888687)\n",
      "Train loss at 2000 is 0.05157649517059326)\n",
      "Train loss at 2000 is 0.05154213309288025)\n",
      "Train loss at 2000 is 0.05150783807039261)\n",
      "Train loss at 2000 is 0.051473576575517654)\n",
      "Train loss at 2000 is 0.05143933743238449)\n",
      "Train loss at 2000 is 0.0514051727950573)\n",
      "Train loss at 2000 is 0.0513710156083107)\n",
      "Train loss at 2000 is 0.051336925476789474)\n",
      "Train loss at 2000 is 0.051302868872880936)\n",
      "Train loss at 2000 is 0.05126885697245598)\n",
      "Train loss at 2000 is 0.0512348897755146)\n",
      "Train loss at 2000 is 0.05120095610618591)\n",
      "Train loss at 2000 is 0.05116705223917961)\n",
      "Train loss at 2000 is 0.05113319307565689)\n",
      "Train loss at 2000 is 0.05109937861561775)\n",
      "Train loss at 2000 is 0.0510656014084816)\n",
      "Train loss at 2000 is 0.05103188753128052)\n",
      "Train loss at 2000 is 0.050998181104660034)\n",
      "Train loss at 2000 is 0.050964515656232834)\n",
      "Train loss at 2000 is 0.05093090608716011)\n",
      "Train loss at 2000 is 0.05089734122157097)\n",
      "Train loss at 2000 is 0.05086381360888481)\n",
      "Train loss at 2000 is 0.050830334424972534)\n",
      "Train loss at 2000 is 0.05079685524106026)\n",
      "Train loss at 2000 is 0.050763439387083054)\n",
      "Train loss at 2000 is 0.050730086863040924)\n",
      "Train loss at 2000 is 0.050696730613708496)\n",
      "Train loss at 2000 is 0.05066346004605293)\n",
      "Train loss at 2000 is 0.050630200654268265)\n",
      "Train loss at 2000 is 0.050596971064805984)\n",
      "Train loss at 2000 is 0.05056380480527878)\n",
      "Train loss at 2000 is 0.05053068324923515)\n",
      "Train loss at 2000 is 0.050497569143772125)\n",
      "Train loss at 2000 is 0.05046452209353447)\n",
      "Train loss at 2000 is 0.050431497395038605)\n",
      "Train loss at 2000 is 0.050398509949445724)\n",
      "Train loss at 2000 is 0.05036557465791702)\n",
      "Train loss at 2000 is 0.05033266544342041)\n",
      "Train loss at 2000 is 0.05029982328414917)\n",
      "Train loss at 2000 is 0.05026698112487793)\n",
      "Train loss at 2000 is 0.050234198570251465)\n",
      "Train loss at 2000 is 0.0502014234662056)\n",
      "Train loss at 2000 is 0.050168734043836594)\n",
      "Train loss at 2000 is 0.05013604834675789)\n",
      "Train loss at 2000 is 0.050103433430194855)\n",
      "Train loss at 2000 is 0.050070833414793015)\n",
      "Train loss at 2000 is 0.05003826692700386)\n",
      "Train loss at 2000 is 0.05000575631856918)\n",
      "Train loss at 2000 is 0.049973271787166595)\n",
      "Train loss at 2000 is 0.04994083195924759)\n",
      "Train loss at 2000 is 0.049908410757780075)\n",
      "Train loss at 2000 is 0.04987605661153793)\n",
      "Train loss at 2000 is 0.049843739718198776)\n",
      "Train loss at 2000 is 0.049811434000730515)\n",
      "Train loss at 2000 is 0.04977918416261673)\n"
     ]
    }
   ],
   "source": [
    "## 신경망 학습\n",
    "for each in range(epochs):\n",
    "    model.train()\n",
    "    # 학습한다. \n",
    "    optimizer.zero_grad()\n",
    "    train_output=model(x_train)\n",
    "    #결과값계싼 \n",
    "    train_loss = criterion(train_output.squeeze(),y_train)\n",
    "    #결과값의 차원을 레이블의 차원과 같게 만들고 오차를 계산한다. \n",
    "    if epochs % 100 == 0:\n",
    "        print('Train loss at {} is {})'.format(epochs, train_loss.item()))\n",
    "    #100 epoch 마다 오차를 출력해 학습이 잘 되는지 확인 \n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    # 신경망의 핵심인 역전파를 행하는 코드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습 후 모델 성능 평가 \n",
    "model.eval()\n",
    "# 모델을 평가 모드로 바꾼다. \n",
    "test_loss = criterion(model(x_test).squeeze(),y_test)\n",
    "# 오차를 구한다. \n",
    "print('After Training, test loss is {}'.format(test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict format of the model :OrderedDict([('linear_1.weight', tensor([[-1.2386,  1.9057],\n",
      "        [ 1.9393, -1.2525],\n",
      "        [ 1.3676,  1.4545],\n",
      "        [-0.3928,  0.5650],\n",
      "        [ 0.4762,  0.6002]])), ('linear_1.bias', tensor([-0.0664, -0.1032, -0.2719, -0.0701, -0.1937])), ('linear_2.weight', tensor([[ 2.2172,  2.2906, -1.8664,  0.5956, -0.6157]])), ('linear_2.bias', tensor([-2.1046]))])\n"
     ]
    }
   ],
   "source": [
    "## 학습된 가중치 저장 \n",
    "torch.save(model.state_dict(),'./model.pt')\n",
    "# state_dict 함수는 모델 내 가중치들이 딕셔너리 형태로 \n",
    "# {연산 이름 : 가중치 tensor와 편향 텐서}와 같이 표현된 데이터 ㅇ\n",
    "print('state_dict format of the model :{}'.format(model.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 [-1,1]이 레이블 1을 가질 확률은 0.9947722554206848\n"
     ]
    }
   ],
   "source": [
    "## 저장된 가중치를 불러와 새로운 모델에 적용 (전이 학습)\n",
    "new_model = NeuralNet(2,5)\n",
    "new_model.load_state_dict(torch.load('./model.pt'))\n",
    "new_model.eval()\n",
    "print('벡터 [-1,1]이 레이블 1을 가질 확률은 {}'.format(new_model(torch.FloatTensor([-1,1])).item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
